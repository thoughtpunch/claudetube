{"id":"claudetube-010","title":"Generate visual transcripts for scenes","description":"## User Story\nAs a user analyzing videos with visual content, I need descriptions of what's happening on screen for each scene.\n\n## Acceptance Criteria\n- [ ] Generates natural language description per scene\n- [ ] Describes: actions, objects, text on screen, people, settings\n- [ ] Uses 1-3 keyframes per scene\n- [ ] Supports both local models and Claude API\n- [ ] Stores in scenes/scene_XXX/visual.json\n\n## Technical Implementation\n\n### Library Options (Choose One):\n\n#### Option A: Claude API (Recommended for accuracy)\n```python\nimport anthropic\n\nclient = anthropic.Anthropic()\n\ndef generate_visual_transcript_claude(keyframe_paths: list[str]) -\u003e dict:\n    # Load images as base64\n    images = [load_image_base64(p) for p in keyframe_paths]\n    \n    response = client.messages.create(\n        model='claude-3-haiku-20240307',  # Fast + cheap\n        max_tokens=500,\n        messages=[{\n            'role': 'user',\n            'content': [\n                *[{'type': 'image', 'source': {'type': 'base64', 'data': img}} \n                  for img in images],\n                {'type': 'text', 'text': '''Describe what is visually happening in these frames.\nFocus on: actions, objects, text on screen, people, settings, changes between frames.\nBe specific and factual. Output JSON with keys: description, people, objects, text_on_screen.'''}\n            ]\n        }]\n    )\n    return json.loads(response.content[0].text)\n```\n\n#### Option B: Molmo 2 (Local, open source, best quality)\n```bash\npip install transformers torch  # Molmo available on HuggingFace\n```\n```python\nfrom transformers import AutoModelForCausalLM, AutoProcessor\n\nmodel = AutoModelForCausalLM.from_pretrained('allenai/Molmo-7B-D-0924')\nprocessor = AutoProcessor.from_pretrained('allenai/Molmo-7B-D-0924')\n```\n\n#### Option C: LLaVA (Local, lighter weight)\n```bash\npip install llava  # Or use transformers\n```\n\n### Recommendation\n1. Default: Claude API (Haiku) - accurate, fast, cheap ($0.001/scene)\n2. Fallback: Local Molmo if no API key or offline mode\n3. Config: CLAUDETUBE_VISION_MODEL=claude|molmo|llava\n\n### Keyframe Selection\n```python\ndef select_keyframes(scene: dict, n: int = 3) -\u003e list[str]:\n    '''Select representative frames from scene.'''\n    duration = scene['end'] - scene['start']\n    \n    if duration \u003c 3:\n        # Short scene: just middle frame\n        return [extract_frame(scene['start'] + duration/2)]\n    \n    # Distribute evenly\n    timestamps = [scene['start'] + i * duration / (n-1) for i in range(n)]\n    return [extract_frame(t) for t in timestamps]\n```","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:18:57.85799-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:58:05.056965-06:00","closed_at":"2026-02-01T11:58:05.056965-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-010","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:35.699241-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-010","depends_on_id":"claudetube-33e","type":"blocks","created_at":"2026-01-31T23:19:44.489586-06:00","created_by":"danielbarrett"}],"comments":[{"id":38,"issue_id":"claudetube-010","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nVisual transcripts (dense captioning) are EXPENSIVE. Follow this hierarchy:\n1. **CACHE** - visual.json exists for this scene? Use it.\n2. **SKIP** - If scene has good transcript coverage, visual transcript may be unnecessary.\n3. **COMPUTE** - Only generate for scenes where visual context adds value.\n\nConsider: Skip visual transcripts for talking-head videos where transcript is sufficient.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:12:12Z"},{"id":59,"issue_id":"claudetube-010","author":"danielbarrett","text":"Commit: 51b05afbdee6cc2d30ab2f57845750c512cc2fa1","created_at":"2026-02-01T17:57:42Z"},{"id":60,"issue_id":"claudetube-010","author":"danielbarrett","text":"## What was done\n- Created visual_transcript.py with generate_visual_transcript() and get_visual_transcript()\n- Implements VisualDescription dataclass with: description, people, objects, text_on_screen, actions, setting\n- Keyframe selection: extracts 1-3 frames per scene at evenly distributed timestamps\n- Smart skip logic: scenes with \u003e2 words/second transcript coverage are skipped (talking heads)\n- Claude Haiku API integration for vision analysis\n- Cache storage in scenes/scene_XXX/visual.json\n- State tracking via visual_transcripts_complete flag\n- Added MCP tool: generate_visual_transcripts\n- Configuration via CLAUDETUBE_VISION_MODEL env var\n\nFiles: src/claudetube/operations/visual_transcript.py (new), src/claudetube/operations/__init__.py, src/claudetube/mcp_server.py\n\n## Left undone\n- Local model support (Molmo, LLaVA) - placeholder in code, returns error message\n\n## Gotchas\n- Requires ANTHROPIC_API_KEY environment variable\n- Uses claude-3-haiku model for cost efficiency (~$0.001/scene)\n- For URL-based videos, downloads segment per-scene for keyframe extraction (cleaned up after)","created_at":"2026-02-01T17:57:56Z"}]}
{"id":"claudetube-0oq","title":"Implement temporal grounding search","description":"## User Story\nAs a user asking 'when do they fix the bug', I want to search for moments matching my query.\n\n## Acceptance Criteria\n- [ ] Accepts natural language query\n- [ ] Returns ranked list of relevant scenes\n- [ ] Includes: scene_id, timestamps, relevance score, preview\n- [ ] Top-k parameter (default 5)\n- [ ] Sub-second response time\n\n## Technical Implementation\n\n### Core Search Function\n```python\nimport voyageai\nimport numpy as np\n\nvoyage = voyageai.Client()\n\ndef find_moments(video_id: str, query: str, top_k: int = 5) -\u003e list[dict]:\n    '''Find scenes matching a natural language query.'''\n    \n    cache_dir = CACHE_BASE / video_id\n    collection = load_scene_index(cache_dir)\n    \n    if collection is None:\n        raise ValueError(f'Video {video_id} not indexed. Run scene embedding first.')\n    \n    # Embed query\n    query_embedding = embed_query(query)\n    \n    # Search\n    results = collection.query(\n        query_embeddings=[query_embedding.tolist()],\n        n_results=top_k,\n        include=['metadatas', 'distances']\n    )\n    \n    # Format results\n    moments = []\n    for i, (id, meta, dist) in enumerate(zip(\n        results['ids'][0],\n        results['metadatas'][0],\n        results['distances'][0]\n    )):\n        moments.append({\n            'rank': i + 1,\n            'scene_id': id,\n            'start': meta['start'],\n            'end': meta['end'],\n            'relevance': 1 - dist,  # Convert distance to similarity\n            'preview': meta['transcript'][:100] + '...',\n            'timestamp_str': format_timestamp(meta['start'])\n        })\n    \n    return moments\n```\n\n### Query Embedding\n```python\ndef embed_query(query: str) -\u003e np.ndarray:\n    '''Embed search query.'''\n    model = os.environ.get('CLAUDETUBE_EMBEDDING_MODEL', 'voyage')\n    \n    if model == 'voyage':\n        result = voyage.multimodal_embed(\n            inputs=[[query]],\n            model='voyage-multimodal-3',\n            input_type='query'  # Important: use query mode\n        )\n        return np.array(result.embeddings[0])\n    else:\n        return text_model.encode(query)\n```\n\n### Format Helpers\n```python\ndef format_timestamp(seconds: float) -\u003e str:\n    '''Convert seconds to MM:SS or HH:MM:SS.'''\n    m, s = divmod(int(seconds), 60)\n    h, m = divmod(m, 60)\n    if h:\n        return f'{h}:{m:02d}:{s:02d}'\n    return f'{m}:{s:02d}'\n```\n\n### Performance\n- Query embedding: ~100ms (Voyage API)\n- ChromaDB search: \u003c10ms (in-memory)\n- Total: \u003c200ms for typical query","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:20:04.289318-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T12:24:27.026995-06:00","closed_at":"2026-02-01T12:24:27.026995-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-0oq","depends_on_id":"claudetube-dth","type":"parent-child","created_at":"2026-01-31T23:20:24.757456-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-0oq","depends_on_id":"claudetube-uzo","type":"blocks","created_at":"2026-01-31T23:20:25.329319-06:00","created_by":"danielbarrett"}],"comments":[{"id":42,"issue_id":"claudetube-0oq","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nTemporal grounding search should be FAST:\n1. **CACHE** - Query results cached? Return them.\n2. **TEXT** - Transcript search is instant, try it first.\n3. **EMBEDDINGS** - Vector similarity only if text search fails.\n\nTarget: \u003c500ms for cached queries, \u003c2s for text search, \u003c5s for embedding search.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:12:39Z"},{"id":65,"issue_id":"claudetube-0oq","author":"danielbarrett","text":"Commit: e3a5d05ffce29acd68eba4ea78391da4baefc12f","created_at":"2026-02-01T18:24:08Z"},{"id":66,"issue_id":"claudetube-0oq","author":"danielbarrett","text":"## What was done\n- Implemented `find_moments()` function for temporal grounding search\n- Added tiered search strategy: text search first, semantic embeddings as fallback\n- Created `SearchMoment` dataclass with rank, scene_id, timestamps, relevance, preview\n- Supports natural language queries like \"when do they fix the bug\"\n- Returns top_k results (default 5) sorted by relevance\n- Added `format_timestamp()` utility for human-readable timestamps (MM:SS or HH:MM:SS)\n- Files: src/claudetube/analysis/search.py, src/claudetube/analysis/__init__.py, tests/test_search.py\n\n## Left undone\n- None\n\n## Gotchas\n- SceneBoundary.to_dict() only includes transcript_text when transcript list is non-empty\n- Tests needed transcript list populated to save/load transcript_text correctly\n- Pre-existing test failures in vocabulary, config_loader modules (unrelated to this task)","created_at":"2026-02-01T18:24:19Z"}]}
{"id":"claudetube-175","title":"Interaction-driven cache enrichment","description":"## User Story\nAs the system, I want to automatically enrich the cache when Claude examines frames or answers questions.\n\n## Acceptance Criteria\n- [ ] Records observations when Claude examines frames\n- [ ] Updates scene metadata with insights\n- [ ] Boosts relevance for examined scenes\n- [ ] Second query is faster than first\n\n## Technical Implementation\n\n### Hook into Frame Examination\n```python\ndef get_frames_at_with_learning(\n    video_id: str,\n    start_time: float,\n    duration: float = 5,\n    **kwargs\n) -\u003e dict:\n    '''Get frames and record that we examined this section.'''\n    \n    # Get frames normally\n    result = get_frames_at(video_id, start_time, duration, **kwargs)\n    \n    # Record examination in memory\n    cache_dir = CACHE_BASE / video_id\n    memory = VideoMemory(video_id, cache_dir)\n    \n    # Find scene containing this timestamp\n    scenes = load_scenes(cache_dir)\n    scene_id = find_scene_at_timestamp(scenes, start_time)\n    \n    if scene_id is not None:\n        memory.record_observation(\n            scene_id=scene_id,\n            obs_type='frames_examined',\n            content=f'Examined frames at {start_time}s for {duration}s'\n        )\n    \n    return result\n```\n\n### Record Q\u0026A from MCP Responses\n```python\ndef answer_question_with_learning(\n    video_id: str,\n    question: str,\n    scenes: list[dict]\n) -\u003e str:\n    '''Answer question and cache for future reference.'''\n    \n    # Generate answer\n    answer = generate_answer(scenes, question)\n    \n    # Find relevant scenes\n    relevant_scene_ids = find_relevant_scenes(scenes, question, answer)\n    \n    # Record Q\u0026A\n    memory = VideoMemory(video_id, CACHE_BASE / video_id)\n    memory.record_qa(question, answer, relevant_scene_ids)\n    \n    return answer\n\ndef find_relevant_scenes(scenes: list, question: str, answer: str) -\u003e list[int]:\n    '''Identify which scenes were used to answer question.'''\n    # Simple: find scenes mentioned in answer or high similarity to question\n    relevant = []\n    \n    for scene in scenes:\n        transcript = scene.get('transcript_text', '').lower()\n        if any(word in transcript for word in question.lower().split() if len(word) \u003e 4):\n            relevant.append(scene['segment_id'])\n    \n    return relevant[:5]  # Top 5\n```\n\n### Relevance Boosting\n```python\ndef boost_scene_relevance(video_id: str, scene_id: int, boost: float = 0.1):\n    '''Boost relevance score for frequently examined scenes.'''\n    \n    cache_dir = CACHE_BASE / video_id\n    state_file = cache_dir / 'scenes' / 'relevance_boosts.json'\n    \n    boosts = json.loads(state_file.read_text()) if state_file.exists() else {}\n    boosts[str(scene_id)] = boosts.get(str(scene_id), 1.0) + boost\n    \n    state_file.write_text(json.dumps(boosts))\n\ndef get_boosted_relevance(video_id: str, scene_id: int, base_relevance: float) -\u003e float:\n    '''Apply boost to search relevance.'''\n    cache_dir = CACHE_BASE / video_id\n    state_file = cache_dir / 'scenes' / 'relevance_boosts.json'\n    \n    if state_file.exists():\n        boosts = json.loads(state_file.read_text())\n        boost = boosts.get(str(scene_id), 1.0)\n        return base_relevance * boost\n    \n    return base_relevance\n```\n\n### Why This Matters\n- First question: full search, generate answer\n- Second question: check QA history, reuse if similar\n- Examined scenes: higher ranking in future searches\n- Progressive refinement without re-processing","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-31T23:21:28.776409-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:32:09.01356-06:00","dependencies":[{"issue_id":"claudetube-175","depends_on_id":"claudetube-9hk","type":"parent-child","created_at":"2026-01-31T23:21:45.820568-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-175","depends_on_id":"claudetube-i3x","type":"blocks","created_at":"2026-01-31T23:21:46.250902-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-1ha","title":"Update MCP server to respect cache configuration","description":"## User Story\nAs a user, I want the MCP server to use my configured cache directory.\n\n## Acceptance Criteria\n- [ ] MCP server reads config on startup\n- [ ] All MCP tools use configured cache directory\n- [ ] Environment variable works when starting MCP server\n\n## Files to modify\n- src/claudetube/mcp_server.py","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-01T10:04:02.024586-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:04:51.41402-06:00","closed_at":"2026-02-01T11:04:51.41402-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-1ha","depends_on_id":"claudetube-dlk","type":"blocks","created_at":"2026-02-01T10:29:43.896758-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-1ha","depends_on_id":"claudetube-dug","type":"parent-child","created_at":"2026-02-01T10:30:37.561823-06:00","created_by":"danielbarrett"}],"comments":[{"id":24,"issue_id":"claudetube-1ha","author":"danielbarrett","text":"Commit: 8fb09095fb553e831b533e1cdc61a601bd9c762c","created_at":"2026-02-01T17:04:32Z"},{"id":25,"issue_id":"claudetube-1ha","author":"danielbarrett","text":"## What was done\n- Replaced hardcoded CACHE_DIR constant with get_cache_dir() function call\n- MCP server now reads cache configuration from config module on each tool invocation\n- Environment variable CLAUDETUBE_CACHE_DIR is respected\n- Project config (.claudetube/config.yaml) and user config (~/.config/claudetube/config.yaml) are also respected\n- Updated test fixture to mock get_cache_dir() instead of CACHE_DIR constant\n- Files modified: src/claudetube/mcp_server.py, tests/test_mcp_server.py\n\n## Left undone\n- None\n\n## Gotchas\n- The config is resolved lazily per-call rather than at startup, which allows dynamic config changes to take effect without restarting the MCP server\n- Tests needed to be updated to mock the function instead of the constant","created_at":"2026-02-01T17:04:44Z"}]}
{"id":"claudetube-2ag","title":"Extract metadata from local files via ffprobe","description":"## User Story\nAs a user processing local videos, I need the system to extract video metadata (duration, dimensions, fps) without relying on yt-dlp.\n\n## Acceptance Criteria\n- [ ] Extracts duration in seconds\n- [ ] Extracts width/height dimensions\n- [ ] Extracts frame rate (fps)\n- [ ] Extracts codec information\n- [ ] Extracts creation_time if available\n- [ ] Graceful error if ffprobe not installed\n- [ ] state.json format matches URL-based videos\n\n## Technical Implementation\n\n### Library: ffmpeg-python (wrapper for ffprobe)\n```bash\npip install ffmpeg-python  # 1.5M+ downloads/month\n```\n\n### Implementation Pattern\n```python\nimport ffmpeg\n\ndef get_local_metadata(file_path: str) -\u003e dict:\n    try:\n        probe = ffmpeg.probe(file_path)\n        video_stream = next(\n            s for s in probe['streams'] if s['codec_type'] == 'video'\n        )\n        \n        return {\n            'duration': float(probe['format']['duration']),\n            'width': video_stream['width'],\n            'height': video_stream['height'],\n            'fps': eval(video_stream['r_frame_rate']),  # e.g., '30/1'\n            'codec': video_stream['codec_name'],\n            'creation_time': probe['format'].get('tags', {}).get('creation_time'),\n            'title': Path(file_path).stem,\n            'uploader': None,\n            'source_type': 'local'\n        }\n    except ffmpeg.Error as e:\n        raise RuntimeError(f'ffprobe failed: {e.stderr.decode()}')\n```\n\n### Alternative: pymediainfo\nIf ffprobe not available, pymediainfo works without ffmpeg installed:\n```bash\npip install pymediainfo  # Uses system MediaInfo library\n```\n\n### Error Handling\n- Check for ffprobe: `shutil.which('ffprobe')`\n- Clear error message: 'ffprobe not found. Install ffmpeg: brew install ffmpeg'","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:16:20.903528-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T09:41:06.081137-06:00","closed_at":"2026-02-01T09:41:06.081137-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-2ag","depends_on_id":"claudetube-2wz","type":"blocks","created_at":"2026-01-31T23:17:13.080353-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-2ag","depends_on_id":"claudetube-u4h","type":"parent-child","created_at":"2026-01-31T23:17:40.064574-06:00","created_by":"danielbarrett"}],"comments":[{"id":3,"issue_id":"claudetube-2ag","author":"danielbarrett","text":"Commit: f9d20e5b5aaadfb08fd0c5629a38a78f0d5e0307","created_at":"2026-02-01T15:40:40Z"},{"id":4,"issue_id":"claudetube-2ag","author":"danielbarrett","text":"## What was done\n- Created FFprobeTool class in tools/ffprobe.py that wraps ffprobe\n- Extracts duration, width, height, fps, codec, creation_time from video files\n- Added format_duration() utility function to utils/formatting.py\n- Updated VideoState.from_local_file() to accept all metadata fields\n- Added VideoMetadata dataclass for structured metadata\n- All 244 tests pass, lint checks pass\n\n## Files changed\n- src/claudetube/tools/ffprobe.py (new)\n- src/claudetube/tools/__init__.py\n- src/claudetube/models/state.py\n- src/claudetube/utils/formatting.py\n\n## Left undone\n- None\n\n## Gotchas\n- No new dependencies needed - ffprobe is called via subprocess (part of ffmpeg)\n- Uses ffprobe -print_format json for reliable parsing\n- Graceful handling when ffprobe not available (returns empty VideoMetadata)","created_at":"2026-02-01T15:40:59Z"}]}
{"id":"claudetube-2pq","title":"Support CLAUDETUBE_CACHE_DIR environment variable","description":"## User Story\nAs a user, I want to set CLAUDETUBE_CACHE_DIR environment variable to control where claudetube stores its data.\n\n## Acceptance Criteria\n- [ ] Check for CLAUDETUBE_CACHE_DIR env var on startup\n- [ ] If set, use that path instead of default\n- [ ] Path can be absolute or relative (resolve to absolute)\n- [ ] Create directory if it doesn't exist\n- [ ] Log which cache directory is being used\n\n## Implementation\n```python\nimport os\nfrom pathlib import Path\n\ndef get_cache_dir() -\u003e Path:\n    # Priority 1: Environment variable\n    if env_dir := os.environ.get('CLAUDETUBE_CACHE_DIR'):\n        return Path(env_dir).expanduser().resolve()\n    # ... other priorities ...\n    # Default\n    return Path.home() / '.claude' / 'video_cache'\n```\n\n## Files to modify\n- src/claudetube/config/defaults.py","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-01T10:03:41.928235-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T10:59:59.133748-06:00","closed_at":"2026-02-01T10:59:59.133748-06:00","close_reason":"Done - CLAUDETUBE_CACHE_DIR environment variable support implemented with path resolution, directory creation, and INFO-level logging","dependencies":[{"issue_id":"claudetube-2pq","depends_on_id":"claudetube-quz","type":"blocks","created_at":"2026-02-01T10:29:43.407728-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-2pq","depends_on_id":"claudetube-dug","type":"parent-child","created_at":"2026-02-01T10:30:36.930192-06:00","created_by":"danielbarrett"}],"comments":[{"id":23,"issue_id":"claudetube-2pq","author":"danielbarrett","text":"## What was done\n- Implemented CLAUDETUBE_CACHE_DIR environment variable support in config/loader.py\n- Path resolution: both absolute and relative paths are converted to absolute via .resolve()\n- Directory creation: get_cache_dir() creates the directory if it doesn't exist (mkdir -p behavior)\n- Logging: INFO-level log message shows exactly which cache directory is being used when env var is set\n- Updated all modules that used CACHE_DIR constant to use get_cache_dir() function:\n  - cache/manager.py\n  - models/video_file.py\n  - operations/processor.py\n  - operations/transcribe.py\n  - operations/extract_frames.py\n- Removed deprecated CACHE_DIR constant from config/defaults.py\n- Files: src/claudetube/config/loader.py, src/claudetube/config/defaults.py, src/claudetube/config/__init__.py, src/claudetube/cache/manager.py, src/claudetube/models/video_file.py, src/claudetube/operations/processor.py, src/claudetube/operations/transcribe.py, src/claudetube/operations/extract_frames.py\n\n## Left undone\n- None\n\n## Gotchas\n- Most of the implementation was already done in claudetube-quz (unified config loader) - this task mainly wired it up to all consumers\n- The import sorting in cache/manager.py needed to be fixed after adding the new import","created_at":"2026-02-01T16:59:50Z"}]}
{"id":"claudetube-2wz","title":"Detect local file paths vs URLs","description":"## User Story\nAs a user with local video files (screen recordings, downloaded videos, offline content), I want claudetube to accept file paths so I can analyze my local videos without uploading them anywhere.\n\n## Acceptance Criteria\n- [ ] Accepts absolute paths: /path/to/video.mp4\n- [ ] Accepts relative paths: ./video.mp4, ../videos/file.mkv\n- [ ] Accepts home-relative paths: ~/Videos/file.mp4\n- [ ] Accepts file:// URIs: file:///path/to/video.mp4\n- [ ] Returns clear error if file doesn't exist\n- [ ] Returns clear error if file is not a video format\n- [ ] Correctly distinguishes URLs from paths (no false positives)\n\n## Technical Implementation\n\n### Library: Use pathlib (stdlib) + validators\n- `pathlib.Path` for path manipulation\n- `validators` package to check if input is URL vs path\n- `mimetypes` (stdlib) for video format detection\n\n### Implementation Pattern\n```python\nfrom pathlib import Path\nimport validators\nimport mimetypes\n\ndef is_local_file(input_str: str) -\u003e bool:\n    # Check if it's a URL first\n    if validators.url(input_str):\n        return False\n    # Check file:// scheme\n    if input_str.startswith('file://'):\n        return True\n    # Check if it resolves to existing file\n    path = Path(input_str).expanduser().resolve()\n    return path.exists() and path.is_file()\n\nSUPPORTED_VIDEO_MIMES = {'video/mp4', 'video/webm', 'video/quicktime', ...}\n```\n\n### Key Decision\n- Add to urls.py as new provider type 'local' in VideoURL class\n- video_id generation delegated to separate ticket\n\n### Dependencies\n- validators\u003e=0.22.0 (pip install validators) - 3M+ downloads/month","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:16:12.480943-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:41:40.643773-06:00","closed_at":"2026-01-31T23:41:40.643773-06:00","close_reason":"Implemented local file path detection in urls.py with full test coverage","dependencies":[{"issue_id":"claudetube-2wz","depends_on_id":"claudetube-u4h","type":"parent-child","created_at":"2026-01-31T23:17:39.814946-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-312","title":"Track code evolution across scenes","description":"## User Story\nAs a user watching coding tutorials, I want to see how code evolves throughout the video.\n\n## Acceptance Criteria\n- [ ] Identifies code units (files, functions) shown on screen\n- [ ] Tracks changes: shown → modified → deleted\n- [ ] Stores version history with timestamps\n- [ ] Enables 'How did the auth middleware evolve?'\n- [ ] Stores in entities/code_evolution.json\n\n## Technical Implementation\n\n### Approach: Diff extracted code across scenes\n```python\nimport difflib\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass CodeSnapshot:\n    scene_id: int\n    timestamp: float\n    content: str\n    language: str\n    change_type: str  # 'shown', 'modified', 'deleted'\n\ndef track_code_evolution(scenes: list[dict]) -\u003e dict[str, List[CodeSnapshot]]:\n    '''Track how code changes across scenes.'''\n    \n    evolution = {}  # code_unit_id -\u003e list of snapshots\n    \n    for scene in scenes:\n        for code_block in scene.get('technical', {}).get('code_blocks', []):\n            unit_id = identify_code_unit(code_block)\n            \n            if unit_id not in evolution:\n                evolution[unit_id] = []\n                change_type = 'shown'\n            else:\n                # Compare to previous version\n                prev = evolution[unit_id][-1]\n                change_type = detect_change_type(prev.content, code_block['content'])\n            \n            evolution[unit_id].append(CodeSnapshot(\n                scene_id=scene['segment_id'],\n                timestamp=scene['start'],\n                content=code_block['content'],\n                language=code_block.get('language', 'unknown'),\n                change_type=change_type\n            ))\n    \n    return evolution\n```\n\n### Code Unit Identification\n```python\nimport re\n\ndef identify_code_unit(code_block: dict) -\u003e str:\n    '''Identify code unit from content (file, function, class).'''\n    content = code_block['content']\n    \n    # Look for function definition\n    func_match = re.search(r'def\\s+(\\w+)\\s*\\(', content)\n    if func_match:\n        return f'function:{func_match.group(1)}'\n    \n    # Look for class definition\n    class_match = re.search(r'class\\s+(\\w+)', content)\n    if class_match:\n        return f'class:{class_match.group(1)}'\n    \n    # Look for file path in comments or IDE chrome\n    file_match = re.search(r'[\\w/]+\\.(py|js|ts|java|go|rs)', content)\n    if file_match:\n        return f'file:{file_match.group(0)}'\n    \n    # Fallback: hash of first line\n    first_line = content.split('\\n')[0][:50]\n    return f'snippet:{hash(first_line) % 10000}'\n```\n\n### Change Detection\n```python\ndef detect_change_type(old_content: str, new_content: str) -\u003e str:\n    '''Detect type of change between code versions.'''\n    \n    if old_content == new_content:\n        return 'unchanged'\n    \n    # Use difflib for detailed comparison\n    diff = list(difflib.unified_diff(\n        old_content.splitlines(),\n        new_content.splitlines()\n    ))\n    \n    additions = sum(1 for line in diff if line.startswith('+') and not line.startswith('+++'))\n    deletions = sum(1 for line in diff if line.startswith('-') and not line.startswith('---'))\n    \n    if additions \u003e 0 and deletions == 0:\n        return 'added_lines'\n    elif deletions \u003e 0 and additions == 0:\n        return 'removed_lines'\n    else:\n        return 'modified'\n```\n\n### Output Format\n```json\n{\n  \"code_evolution\": {\n    \"function:validate_token\": [\n      {\"timestamp\": 120.5, \"change_type\": \"shown\", \"content\": \"def validate_token(token):...\"},\n      {\"timestamp\": 245.2, \"change_type\": \"modified\", \"content\": \"def validate_token(token, check_expiry=True):...\"},\n      {\"timestamp\": 380.1, \"change_type\": \"added_lines\", \"content\": \"def validate_token(token, check_expiry=True):\\n    if check_expiry...\"}\n    ]\n  }\n}\n```","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:20:39.709607-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:30:25.669446-06:00","dependencies":[{"issue_id":"claudetube-312","depends_on_id":"claudetube-sa5","type":"parent-child","created_at":"2026-01-31T23:21:04.481908-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-312","depends_on_id":"claudetube-mnq","type":"blocks","created_at":"2026-01-31T23:21:05.080861-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-33e","title":"Align transcript to detected scenes","description":"## User Story\nAs a user asking 'what did they say when showing the diagram', I need transcript text linked to video segments.\n\n## Acceptance Criteria\n- [ ] Maps each transcript segment to its containing scene\n- [ ] Each scene gets transcript list + joined transcript_text\n- [ ] Handles edge cases (transcript spans scene boundary)\n- [ ] Preserves original timestamps\n\n## Technical Implementation\n\n### Simple Interval Matching\nNo external library needed - just interval math.\n\n```python\ndef align_transcript_to_scenes(\n    transcript_segments: list[dict],\n    scenes: list[dict]\n) -\u003e list[dict]:\n    '''Map transcript segments to their containing scenes.'''\n    \n    for scene in scenes:\n        scene['transcript'] = []\n        scene['transcript_text'] = ''\n    \n    for seg in transcript_segments:\n        seg_mid = (seg['start'] + seg['end']) / 2\n        \n        # Find containing scene (by midpoint)\n        for scene in scenes:\n            if scene['start'] \u003c= seg_mid \u003c scene['end']:\n                scene['transcript'].append(seg)\n                break\n    \n    # Join transcript text\n    for scene in scenes:\n        scene['transcript_text'] = ' '.join(\n            seg['text'] for seg in scene['transcript']\n        )\n    \n    return scenes\n```\n\n### Alternative: Binary Search for Large Videos\n```python\nimport bisect\n\ndef align_transcript_to_scenes_fast(\n    transcript_segments: list[dict],\n    scenes: list[dict]\n) -\u003e list[dict]:\n    '''O(n log m) alignment for large videos.'''\n    \n    # Pre-compute scene boundaries for binary search\n    scene_starts = [s['start'] for s in scenes]\n    \n    for scene in scenes:\n        scene['transcript'] = []\n    \n    for seg in transcript_segments:\n        seg_mid = (seg['start'] + seg['end']) / 2\n        idx = bisect.bisect_right(scene_starts, seg_mid) - 1\n        if 0 \u003c= idx \u003c len(scenes):\n            scenes[idx]['transcript'].append(seg)\n    \n    for scene in scenes:\n        scene['transcript_text'] = ' '.join(\n            seg['text'] for seg in scene['transcript']\n        )\n    \n    return scenes\n```\n\n### Edge Case: Segment Spans Boundary\nOption A: Assign to scene containing midpoint (current)\nOption B: Split segment and assign portions to each scene\nOption C: Assign to scene with more overlap\n\nRecommendation: Option A (midpoint) - simple and usually correct","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:18:53.262994-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:43:08.779812-06:00","closed_at":"2026-02-01T11:43:08.779812-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-33e","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:35.598862-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-33e","depends_on_id":"claudetube-vs1","type":"blocks","created_at":"2026-01-31T23:19:44.386459-06:00","created_by":"danielbarrett"}],"comments":[{"id":52,"issue_id":"claudetube-33e","author":"danielbarrett","text":"Commit: 7a95e182cfd26b666e2f246ec636a6f5da1ff6af","created_at":"2026-02-01T17:42:48Z"},{"id":53,"issue_id":"claudetube-33e","author":"danielbarrett","text":"## What was done\n- Implemented align_transcript_to_scenes() using binary search for O(n log m) alignment\n- Each segment is assigned to the scene containing its midpoint\n- SceneBoundary now has transcript (list) and transcript_text (joined string) fields\n- Integrated alignment into segment_video_smart - runs automatically when transcript_segments provided\n- Added 20 tests covering all edge cases: empty inputs, boundary spanning, timestamps preservation\n\n## Files modified\n- src/claudetube/analysis/alignment.py (NEW)\n- src/claudetube/analysis/__init__.py\n- src/claudetube/cache/scenes.py\n- src/claudetube/operations/segmentation.py\n- tests/test_alignment.py (NEW)\n- tests/test_scenes.py\n\n## Left undone\n- None\n\n## Gotchas\n- Midpoint matching (Option A from spec) is the best tradeoff for simplicity vs accuracy\n- to_dict() now omits empty transcript fields for sparse output (test updated accordingly)\n- Original segment timestamps fully preserved in the transcript list","created_at":"2026-02-01T17:43:01Z"}]}
{"id":"claudetube-3br","title":"Document cache directory configuration options","description":"## User Story\nAs a user, I need documentation on how to configure the cache directory.\n\n## Acceptance Criteria\n- [ ] Document environment variable option\n- [ ] Document project config option\n- [ ] Document user config option\n- [ ] Document priority order\n- [ ] Add examples for common use cases\n- [ ] Update architecture docs\n\n## Files to modify/create\n- documentation/guides/configuration.md (new)\n- documentation/architecture/cache.md\n- README.md (mention config options)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-01T10:04:06.074378-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:48:53.644677-06:00","closed_at":"2026-02-01T11:48:53.644677-06:00","close_reason":"Created comprehensive configuration guide covering ENV, project config, user config, and priority order. Updated README.md with config section.","dependencies":[{"issue_id":"claudetube-3br","depends_on_id":"claudetube-1ha","type":"blocks","created_at":"2026-02-01T10:29:44.019148-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-3br","depends_on_id":"claudetube-dug","type":"parent-child","created_at":"2026-02-01T10:30:37.688686-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-3dt","title":"Unified cheap boundary detection","description":"## User Story\nAs a developer, I need a unified function that combines all cheap boundary detection methods before falling back to expensive visual analysis.\n\n## Acceptance Criteria\n- [ ] Calls: chapters, linguistic, pauses, vocabulary shifts\n- [ ] Merges nearby boundaries (\u003c5s) keeping highest confidence\n- [ ] Boosts confidence when multiple signals agree\n- [ ] Returns sorted list by timestamp\n- [ ] Fast: \u003c2s for 30-min video\n\n## Technical Implementation\n\n### Design Pattern: Chain of Responsibility\nEach detector is independent, results are merged.\n\n```python\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass Boundary:\n    timestamp: float\n    type: str\n    confidence: float\n    sources: List[str]\n\ndef detect_boundaries_cheap(\n    video_info: dict,\n    transcript_segments: list,\n    srt_path: str\n) -\u003e List[Boundary]:\n    all_boundaries = []\n    \n    # 1. YouTube chapters (highest confidence)\n    chapters = extract_youtube_chapters(video_info)\n    all_boundaries.extend([\n        Boundary(c['start'], 'chapter', c['confidence'], [c['source']])\n        for c in chapters\n    ])\n    \n    # 2. Linguistic transitions\n    linguistic = detect_linguistic_boundaries(transcript_segments)\n    all_boundaries.extend([\n        Boundary(b.timestamp, 'linguistic', b.confidence, ['linguistic_cue'])\n        for b in linguistic\n    ])\n    \n    # 3. Pauses\n    pauses = detect_pause_boundaries(srt_path)\n    all_boundaries.extend([\n        Boundary(p['timestamp'], 'pause', p['confidence'], ['pause'])\n        for p in pauses\n    ])\n    \n    # 4. Vocabulary shifts\n    vocab = detect_vocabulary_shifts(transcript_segments)\n    all_boundaries.extend([\n        Boundary(v['timestamp'], 'vocabulary', v['confidence'], ['vocabulary_shift'])\n        for v in vocab\n    ])\n    \n    # Merge nearby boundaries\n    return merge_nearby_boundaries(all_boundaries, threshold=5.0)\n\ndef merge_nearby_boundaries(\n    boundaries: List[Boundary],\n    threshold: float = 5.0\n) -\u003e List[Boundary]:\n    if not boundaries:\n        return []\n    \n    sorted_b = sorted(boundaries, key=lambda x: x.timestamp)\n    merged = [sorted_b[0]]\n    \n    for b in sorted_b[1:]:\n        if b.timestamp - merged[-1].timestamp \u003c threshold:\n            # Merge: keep earlier timestamp, boost confidence, combine sources\n            merged[-1].confidence = min(merged[-1].confidence + 0.1, 0.95)\n            merged[-1].sources.extend(b.sources)\n        else:\n            merged.append(b)\n    \n    return merged\n```\n\n### Performance Target\n- \u003c2 seconds for typical 30-min video\n- All processing is text-based (no video decoding)\n- sklearn TF-IDF is the bottleneck (~500ms)","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:18:40.038778-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:25:03.304438-06:00","closed_at":"2026-02-01T11:25:03.304438-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-3dt","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:35.283296-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-3dt","depends_on_id":"claudetube-jmu","type":"blocks","created_at":"2026-01-31T23:19:43.730274-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-3dt","depends_on_id":"claudetube-qgi","type":"blocks","created_at":"2026-01-31T23:19:43.840048-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-3dt","depends_on_id":"claudetube-t10","type":"blocks","created_at":"2026-01-31T23:19:43.960367-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-3dt","depends_on_id":"claudetube-4fh","type":"blocks","created_at":"2026-01-31T23:19:44.063031-06:00","created_by":"danielbarrett"}],"comments":[{"id":32,"issue_id":"claudetube-3dt","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nUnified boundary detection is the CHEAP path before visual analysis:\n1. **CHAPTERS** - From yt-dlp, highest confidence (0.95).\n2. **DESCRIPTION** - Parsed timestamps, high confidence (0.9).\n3. **LINGUISTIC** - Transition phrases in transcript (~1s).\n4. **PAUSES** - Long silences in transcript (~0.5s).\n5. **VOCABULARY** - Topic shifts in word usage (~1s).\n\nALL of these are fast (\u003c2s total). Only after this fails should visual detection run.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:11:32Z"},{"id":47,"issue_id":"claudetube-3dt","author":"danielbarrett","text":"## What was done\n- Created `src/claudetube/analysis/unified.py` with `detect_boundaries_cheap()` function\n- Implements chain of responsibility pattern: chapters → linguistic → pauses → vocabulary\n- Added `merge_nearby_boundaries()` function that merges boundaries \u003c5s apart\n- Boosts confidence when multiple signals agree (capped at 0.95)\n- Updated `src/claudetube/analysis/__init__.py` with new exports\n- Added comprehensive tests in `tests/test_unified.py` (34 tests, all passing)\n\n## Files modified\n- src/claudetube/analysis/unified.py (NEW - 179 lines)\n- src/claudetube/analysis/__init__.py (updated exports)\n- tests/test_unified.py (NEW - 314 lines)\n\n## Left undone\n- None\n\n## Gotchas\n- Confidence capping at 0.95 means chapter boundaries (already at 0.95) don't increase when merged\n- TF-IDF vocabulary analysis remains the performance bottleneck (~500ms)\n- All other operations are text-based and fast","created_at":"2026-02-01T17:24:56Z"}]}
{"id":"claudetube-3yz","title":"Copy or symlink local files to cache","description":"## User Story\nAs a user, I want to choose whether to symlink (save space) or copy (self-contained cache) my local video files to the cache directory.\n\n## Acceptance Criteria\n- [ ] Default: symlink to original file (no disk duplication)\n- [ ] Optional --copy flag: copy file to cache\n- [ ] Symlink stored as 'source.EXT' (preserving extension)\n- [ ] state.json tracks cache_mode (symlink|copy)\n- [ ] Warning if source file is moved/deleted (broken symlink)\n- [ ] Works cross-platform (Windows junctions if needed)\n\n## Technical Implementation\n\n### Library: pathlib (stdlib) + shutil (stdlib)\nNo external dependencies needed.\n\n```python\nfrom pathlib import Path\nimport shutil\nimport os\n\ndef cache_local_file(source: Path, cache_dir: Path, copy: bool = False) -\u003e Path:\n    dest = cache_dir / f'source{source.suffix}'\n    \n    if copy:\n        shutil.copy2(source, dest)  # Preserves metadata\n        mode = 'copy'\n    else:\n        # Symlink (or junction on Windows)\n        if dest.exists():\n            dest.unlink()\n        dest.symlink_to(source.resolve())\n        mode = 'symlink'\n    \n    return dest, mode\n```\n\n### Cross-Platform Note\n- macOS/Linux: `os.symlink()` works natively\n- Windows: Requires admin rights OR developer mode enabled\n- Fallback: Copy if symlink fails on Windows\n\n### state.json Addition\n```json\n{\n  \"source_type\": \"local\",\n  \"source_path\": \"/original/path/video.mp4\",\n  \"cache_mode\": \"symlink\",  // or \"copy\"\n  \"cached_file\": \"source.mp4\"\n}\n```","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:16:24.923162-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T10:01:11.033998-06:00","closed_at":"2026-02-01T10:01:11.033998-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-3yz","depends_on_id":"claudetube-2wz","type":"blocks","created_at":"2026-01-31T23:17:13.194722-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-3yz","depends_on_id":"claudetube-u4h","type":"parent-child","created_at":"2026-01-31T23:17:40.229407-06:00","created_by":"danielbarrett"}],"comments":[{"id":11,"issue_id":"claudetube-3yz","author":"danielbarrett","text":"Commit: 42dfecda5472209f69d5e19206f508830b3f14dc","created_at":"2026-02-01T16:00:44Z"},{"id":12,"issue_id":"claudetube-3yz","author":"danielbarrett","text":"## What was done\n- Added `cache_mode` and `cached_file` fields to VideoState model\n- Created `cache_local_file()` function: symlink by default, copy optional\n- Created `check_cached_source()` function: detects broken symlinks\n- Added CacheManager methods: `cache_local_file`, `get_source_path`, `check_source_valid`\n- Cross-platform Windows symlink fallback to copy\n- Files: `models/state.py`, `cache/storage.py`, `cache/manager.py`, `cache/__init__.py`, `tests/test_cache_local_file.py`\n\n## Left undone\n- None\n\n## Gotchas\n- Windows requires admin rights or developer mode for symlinks; fallback copies automatically","created_at":"2026-02-01T16:01:04Z"}]}
{"id":"claudetube-4f1","title":"Generate thumbnail from local video","description":"## User Story\nAs a user processing local videos, I want a thumbnail generated automatically for visual identification in the cache listing.\n\n## Acceptance Criteria\n- [ ] Generates thumbnail.jpg in cache directory\n- [ ] Extracts frame at ~10% of duration or 5 seconds (whichever is less)\n- [ ] Scales to reasonable size (640px width)\n- [ ] Handles audio-only files gracefully (no thumbnail)\n- [ ] Sets has_thumbnail in state.json\n\n## Technical Implementation\n\n### Library: ffmpeg-python\nAlready a dependency.\n\n```python\nimport ffmpeg\n\ndef generate_thumbnail_local(video_path: Path, output_dir: Path, duration: float) -\u003e bool:\n    output = output_dir / 'thumbnail.jpg'\n    \n    # Pick timestamp: 10% of duration or 5s, whichever is less\n    timestamp = min(duration * 0.1, 5.0)\n    \n    try:\n        (\n            ffmpeg\n            .input(str(video_path), ss=timestamp)\n            .filter('scale', 640, -1)\n            .output(str(output), vframes=1, qscale=2)\n            .overwrite_output()\n            .run(capture_stdout=True, capture_stderr=True)\n        )\n        return True\n    except ffmpeg.Error:\n        # Likely audio-only file\n        return False\n```\n\n### Edge Cases\n- Audio-only: Return False, set has_thumbnail=false in state.json\n- Very short videos: Use 0.5s timestamp minimum\n- HDR content: Let ffmpeg handle tone mapping automatically","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:16:40.920919-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T10:47:21.335859-06:00","closed_at":"2026-02-01T10:47:21.335859-06:00","close_reason":"Done (implemented in claudetube-lk8)","dependencies":[{"issue_id":"claudetube-4f1","depends_on_id":"claudetube-3yz","type":"blocks","created_at":"2026-01-31T23:17:13.75345-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-4f1","depends_on_id":"claudetube-u4h","type":"parent-child","created_at":"2026-01-31T23:17:40.889553-06:00","created_by":"danielbarrett"}],"comments":[{"id":19,"issue_id":"claudetube-4f1","author":"danielbarrett","text":"## What was done\nImplemented as part of claudetube-lk8 (process_local_file MCP tool).\n\nThumbnail generation in `process_local_video()`:\n- Generates thumbnail.jpg in cache directory\n- Extracts frame at `min(duration * 0.1, 5.0)` seconds\n- Scales to 480px width (reasonable size)\n- Skips gracefully for audio-only files (`local_file.is_video` check)\n- Sets `has_thumbnail=True` in state.json\n\nSee commit: f2dba5b (claudetube-lk8)\n\n## Left undone\n- None\n\n## Gotchas\n- Uses FFmpegTool.extract_frame() which was already available","created_at":"2026-02-01T16:47:21Z"}]}
{"id":"claudetube-4fh","title":"Detect vocabulary shifts between segments","description":"## User Story\nAs a user analyzing long videos, I want automatic detection of when the vocabulary suddenly changes (indicating new topic).\n\n## Acceptance Criteria\n- [ ] Groups transcript into 30-second windows\n- [ ] Computes TF-IDF vectors per window\n- [ ] Detects low similarity (\u003c0.3) = vocabulary shift\n- [ ] Returns keywords_before and keywords_after for context\n- [ ] Confidence=0.6 (moderate - vocabulary shifts aren't always topic changes)\n\n## Technical Implementation\n\n### Library: scikit-learn\n```bash\npip install scikit-learn  # 15M+ downloads/month - THE standard ML library\n```\n\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndef detect_vocabulary_shifts(\n    transcript_segments: list[dict],\n    window_seconds: int = 30\n) -\u003e list[dict]:\n    # Group into time windows\n    windows = group_into_windows(transcript_segments, window_seconds)\n    \n    if len(windows) \u003c 2:\n        return []\n    \n    # Compute TF-IDF\n    vectorizer = TfidfVectorizer(\n        stop_words='english',\n        max_features=100,\n        ngram_range=(1, 2)  # Unigrams and bigrams\n    )\n    texts = [w['text'] for w in windows]\n    tfidf = vectorizer.fit_transform(texts)\n    \n    boundaries = []\n    feature_names = vectorizer.get_feature_names_out()\n    \n    for i in range(1, len(windows)):\n        sim = cosine_similarity(tfidf[i-1], tfidf[i])[0][0]\n        \n        if sim \u003c 0.3:  # Topic shift threshold\n            boundaries.append({\n                'timestamp': windows[i]['start'],\n                'type': 'vocabulary_shift',\n                'similarity': float(sim),\n                'confidence': 0.6,\n                'keywords_before': get_top_terms(tfidf[i-1], feature_names, 5),\n                'keywords_after': get_top_terms(tfidf[i], feature_names, 5)\n            })\n    \n    return boundaries\n\ndef get_top_terms(tfidf_row, feature_names: np.ndarray, n: int) -\u003e list[str]:\n    indices = tfidf_row.toarray()[0].argsort()[-n:][::-1]\n    return [feature_names[i] for i in indices]\n\ndef group_into_windows(segments: list[dict], window_sec: int) -\u003e list[dict]:\n    windows = []\n    current_window = {'start': 0, 'text': '', 'segments': []}\n    \n    for seg in segments:\n        if seg['start'] \u003e= current_window['start'] + window_sec:\n            if current_window['text']:\n                windows.append(current_window)\n            current_window = {'start': seg['start'], 'text': '', 'segments': []}\n        current_window['text'] += ' ' + seg['text']\n        current_window['segments'].append(seg)\n    \n    if current_window['text']:\n        windows.append(current_window)\n    \n    return windows\n```\n\n### Why TF-IDF\n- Fast, no external API needed\n- Captures 'rare but important' words\n- scikit-learn is battle-tested","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:18:35.87417-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:19:46.62443-06:00","closed_at":"2026-02-01T11:19:46.62443-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-4fh","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:35.169278-06:00","created_by":"danielbarrett"}],"comments":[{"id":35,"issue_id":"claudetube-4fh","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nVocabulary shift detection is part of the CHEAP transcript analysis chain:\n- Input: Already-fetched transcript text (no additional I/O)\n- Processing: Sliding window word frequency analysis\n- Target latency: \u003c1s for 30-min video\n\nThis runs BEFORE any visual analysis. Cache results in scenes.json.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:11:52Z"},{"id":46,"issue_id":"claudetube-4fh","author":"danielbarrett","text":"## What was done\n- Created `src/claudetube/analysis/vocabulary.py` with TF-IDF-based vocabulary shift detection\n- Uses 30-second sliding windows to compute TF-IDF vectors\n- Detects shifts when cosine similarity drops below 0.3 threshold\n- Returns top keywords from before/after windows for context\n- Confidence fixed at 0.6 (moderate - vocabulary shifts aren't always topic changes)\n- Added scikit-learn\u003e=1.0.0 to dependencies in pyproject.toml\n- Exported `detect_vocabulary_shifts` from analysis module\n- Created comprehensive test suite in `tests/test_vocabulary.py` (27 tests)\n\nFiles modified:\n- `pyproject.toml` - added scikit-learn dependency\n- `src/claudetube/analysis/__init__.py` - export new function\n- `src/claudetube/analysis/vocabulary.py` - new module (created)\n- `tests/test_vocabulary.py` - test suite (created)\n\n## Left undone\n- None\n\n## Gotchas\n- TF-IDF vectorizer strips English stopwords, so all-stopword windows produce empty vocabulary\n- Trigger text is truncated to 50 chars for Boundary compatibility\n- Uses lazy import of sklearn to provide clear error message if not installed\n- Commit: 4317b4d85930ba11cf9953952a91eeb29b79b4a4","created_at":"2026-02-01T17:19:40Z"}]}
{"id":"claudetube-4wc","title":"EPIC: Phase 5 - Human-Like Video Comprehension","description":"The agent watches video like a human expert would. Core capabilities:\n- Active watching strategy (decide where to focus)\n- Attention modeling (prioritize what matters)\n- Comprehension verification (self-check understanding)\n\nThis is the ultimate goal: true video comprehension, not just retrieval.\n\n## Success Criteria\n- [ ] ActiveVideoWatcher class implements watch strategy\n- [ ] Agent decides which scenes need deeper analysis\n- [ ] Attention model prioritizes based on user query\n- [ ] Comprehension checks verify understanding before answering\n- [ ] /yt:watch command enables active viewing mode","status":"open","priority":3,"issue_type":"epic","created_at":"2026-01-31T23:18:09.700562-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T09:54:52.334414-06:00"}
{"id":"claudetube-4y8","title":"Generate video_id for local files","description":"## User Story\nAs a developer, I need a consistent video_id for local files so the cache system works identically for local and remote videos.\n\n## Acceptance Criteria\n- [ ] video_id is filesystem-safe (alphanumeric + hyphens + underscores)\n- [ ] video_id is deterministic (same file = same ID)\n- [ ] video_id avoids collisions (different files = different IDs)\n- [ ] video_id is reasonably short (\u003c50 chars)\n- [ ] Original file path stored in state.json for reference\n\n## Technical Implementation\n\n### Strategy: Filename + Hash Suffix\nUse sanitized filename + first 8 chars of path hash for uniqueness:\n```python\nimport hashlib\nimport re\nfrom pathlib import Path\n\ndef generate_local_video_id(file_path: str) -\u003e str:\n    path = Path(file_path).expanduser().resolve()\n    \n    # Sanitize filename (remove extension, replace unsafe chars)\n    name = path.stem\n    safe_name = re.sub(r'[^a-zA-Z0-9_-]', '_', name)[:30]\n    \n    # Add hash suffix for uniqueness\n    path_hash = hashlib.sha256(str(path).encode()).hexdigest()[:8]\n    \n    return f\"{safe_name}_{path_hash}\"\n    # e.g., 'my_screen_recording_a3f2dd1e'\n```\n\n### Why This Approach\n- Human-readable (you can see the filename)\n- Collision-resistant (hash suffix)\n- Deterministic (same path = same ID)\n- No external dependencies\n\n### state.json Addition\n```json\n{\n  \"video_id\": \"my_recording_a3f2dd1e\",\n  \"source_type\": \"local\",\n  \"source_path\": \"/absolute/path/to/my_recording.mp4\",\n  \"url\": null\n}\n```","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:16:16.860772-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T09:35:06.111711-06:00","closed_at":"2026-02-01T09:35:06.111711-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-4y8","depends_on_id":"claudetube-2wz","type":"blocks","created_at":"2026-01-31T23:17:12.952262-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-4y8","depends_on_id":"claudetube-u4h","type":"parent-child","created_at":"2026-01-31T23:17:39.94538-06:00","created_by":"danielbarrett"}],"comments":[{"id":1,"issue_id":"claudetube-4y8","author":"danielbarrett","text":"Commit: f53882d0a3fae4a5283fdafc802e12696a86d196","created_at":"2026-02-01T15:34:38Z"},{"id":2,"issue_id":"claudetube-4y8","author":"danielbarrett","text":"## What was done\n- Added `LocalFile.video_id` property that generates deterministic, filesystem-safe IDs\n- Format: `sanitized_name_8charHash` (e.g., `my_recording_a3f2dd1e`)\n- Added `source_type` and `source_path` fields to `VideoState`\n- Added `VideoState.from_local_file()` factory method\n- Added 12 comprehensive tests for video_id generation and VideoState\n\n## Files changed\n- `src/claudetube/models/local_file.py` - Added video_id property\n- `src/claudetube/models/state.py` - Added source_type, source_path, from_local_file()\n- `tests/test_urls.py` - Added TestLocalFileVideoId and TestVideoStateLocalFile\n\n## Left undone\n- None\n\n## Gotchas\n- None, implementation was straightforward","created_at":"2026-02-01T15:34:58Z"}]}
{"id":"claudetube-5d3","title":"Track people across scenes","description":"## User Story\nAs a user watching videos with multiple people, I want to track who appears when and what they're doing.\n\n## Acceptance Criteria\n- [ ] Identifies distinct people across scenes\n- [ ] Tracks appearances: scene_id, timestamp, action\n- [ ] Maintains person_id consistency across video\n- [ ] Stores in entities/people.json\n- [ ] Works without training on specific faces\n\n## Technical Implementation\n\n### Library Options:\n\n#### Option A: face_recognition (Recommended - Simple API)\n```bash\npip install face_recognition  # 10M+ downloads, built on dlib\n```\n\n```python\nimport face_recognition\nimport numpy as np\nfrom collections import defaultdict\n\nclass PersonTracker:\n    def __init__(self, tolerance: float = 0.6):\n        self.tolerance = tolerance\n        self.known_encodings = []\n        self.known_ids = []\n        self.appearances = defaultdict(list)\n    \n    def process_frame(self, frame_path: str, scene_id: int, timestamp: float):\n        image = face_recognition.load_image_file(frame_path)\n        encodings = face_recognition.face_encodings(image)\n        \n        for encoding in encodings:\n            person_id = self._match_or_create(encoding)\n            self.appearances[person_id].append({\n                'scene_id': scene_id,\n                'timestamp': timestamp,\n            })\n    \n    def _match_or_create(self, encoding: np.ndarray) -\u003e str:\n        if self.known_encodings:\n            distances = face_recognition.face_distance(self.known_encodings, encoding)\n            min_idx = np.argmin(distances)\n            \n            if distances[min_idx] \u003c self.tolerance:\n                return self.known_ids[min_idx]\n        \n        # New person\n        new_id = f'person_{len(self.known_ids)}'\n        self.known_encodings.append(encoding)\n        self.known_ids.append(new_id)\n        return new_id\n```\n\n#### Option B: deepface (More features, heavier)\n```bash\npip install deepface  # Multiple backends, face analysis\n```\n\n### Lighter Alternative: CLIP-based matching\nIf face_recognition is too heavy:\n```python\n# Use CLIP to embed face crops, cluster similar embeddings\nfrom sklearn.cluster import DBSCAN\n\ndef cluster_faces_clip(face_embeddings: list[np.ndarray]) -\u003e list[int]:\n    clustering = DBSCAN(eps=0.5, min_samples=2)\n    return clustering.fit_predict(face_embeddings)\n```\n\n### Action Extraction\n```python\ndef extract_action(scene: dict, person_id: str) -\u003e str:\n    '''Extract what person is doing from visual description.'''\n    visual = scene.get('visual', {}).get('description', '')\n    \n    # Simple keyword extraction\n    action_keywords = ['typing', 'speaking', 'pointing', 'writing', \n                       'presenting', 'demonstrating', 'explaining']\n    \n    for action in action_keywords:\n        if action in visual.lower():\n            return action\n    \n    return 'present'\n```\n\n### Output Format\n```json\n{\n  \"people\": {\n    \"person_0\": {\n      \"appearances\": [\n        {\"scene_id\": 0, \"timestamp\": 5.2, \"action\": \"speaking\"},\n        {\"scene_id\": 3, \"timestamp\": 45.8, \"action\": \"typing\"}\n      ],\n      \"total_screen_time\": 320.5\n    }\n  }\n}\n```","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:20:35.348288-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:30:11.241153-06:00","dependencies":[{"issue_id":"claudetube-5d3","depends_on_id":"claudetube-sa5","type":"parent-child","created_at":"2026-01-31T23:21:04.362362-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-5d3","depends_on_id":"claudetube-010","type":"blocks","created_at":"2026-01-31T23:21:04.965623-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-6g0","title":"Frame extraction from local files","description":"## User Story\nAs a user analyzing local videos, I want to extract frames at specific timestamps without downloading anything (it's already local).\n\n## Acceptance Criteria\n- [ ] Extracts frames at specified timestamp range\n- [ ] Supports quality tiers: lowest (480p) to highest (1080p)\n- [ ] Outputs to drill_QUALITY/ directory\n- [ ] Frame naming: frame_MM-SS.jpg\n- [ ] Significantly faster than URL-based (no download step)\n- [ ] Handles seeking accurately (keyframe vs exact)\n\n## Technical Implementation\n\n### Library: ffmpeg-python\nAlready a dependency - no additional installs.\n\n```python\nimport ffmpeg\n\ndef extract_frames_local(\n    video_path: Path,\n    output_dir: Path,\n    start_time: float,\n    duration: float = 5,\n    interval: float = 1,\n    width: int = 480\n) -\u003e list[Path]:\n    frames_dir = output_dir / f'drill_{width}'\n    frames_dir.mkdir(exist_ok=True)\n    \n    # Calculate number of frames\n    n_frames = int(duration / interval) + 1\n    \n    # Use ffmpeg with fast seeking\n    output_pattern = str(frames_dir / 'frame_%02d.jpg')\n    \n    (\n        ffmpeg\n        .input(str(video_path), ss=start_time)\n        .filter('fps', fps=1/interval)\n        .filter('scale', width, -1)\n        .output(output_pattern, vframes=n_frames, qscale=2)\n        .overwrite_output()\n        .run(capture_stdout=True, capture_stderr=True)\n    )\n    \n    return list(frames_dir.glob('frame_*.jpg'))\n```\n\n### Performance Notes\n- `-ss` before `-i` = fast seek (keyframe-based)\n- `-ss` after `-i` = accurate seek (slower but precise)\n- For frame extraction, keyframe seek is usually fine\n\n### Quality Mapping\n```python\nQUALITY_WIDTHS = {\n    'lowest': 480,\n    'low': 640,\n    'medium': 854,\n    'high': 1280,\n    'highest': 1920\n}\n```","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:16:36.942603-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T10:06:08.606554-06:00","closed_at":"2026-02-01T10:06:08.606554-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-6g0","depends_on_id":"claudetube-3yz","type":"blocks","created_at":"2026-01-31T23:17:13.606066-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-6g0","depends_on_id":"claudetube-u4h","type":"parent-child","created_at":"2026-01-31T23:17:40.754988-06:00","created_by":"danielbarrett"}],"comments":[{"id":13,"issue_id":"claudetube-6g0","author":"danielbarrett","text":"Commit: 9ddfb0d8077e90bdc41c45b9e070d97c9050a086","created_at":"2026-02-01T16:05:49Z"},{"id":14,"issue_id":"claudetube-6g0","author":"danielbarrett","text":"## What was done\n- Added `extract_frames_local()` function for extracting frames from cached local video files\n- Added `extract_hq_frames_local()` function for high-quality frame extraction from local files\n- Added `QUALITY_WIDTHS` mapping: lowest=480, low=640, medium=854, high=1280, highest=1920\n- Updated operations/__init__.py to export new functions\n- Added comprehensive test suite (16 tests)\n\nFiles:\n- src/claudetube/operations/extract_frames.py (added 193 lines)\n- src/claudetube/operations/__init__.py (exports)\n- tests/test_extract_frames_local.py (new, 350 lines)\n\n## Left undone\n- None\n\n## Gotchas\n- Local frame extraction uses seek_offset=0.0 since we're seeking directly in the source file (no segment download)\n- Frame prefix is \"frame\" for drill and \"hq\" for high-quality to match expected patterns\n- state.json tracking includes \"local\": true flag to distinguish from URL extractions","created_at":"2026-02-01T16:06:02Z"}]}
{"id":"claudetube-74k","title":"Tests for local file processing","description":"## User Story\nAs a developer, I need comprehensive tests to ensure local file processing works correctly and doesn't regress.\n\n## Acceptance Criteria\n- [ ] Unit tests for path detection (absolute, relative, ~, file://)\n- [ ] Unit tests for video_id generation\n- [ ] Unit tests for metadata extraction\n- [ ] Integration test: full local file pipeline\n- [ ] Tests pass in CI (GitHub Actions)\n- [ ] Test fixtures: small sample video in repo or downloaded\n\n## Technical Implementation\n\n### Library: pytest + pytest-mock\n```bash\npip install pytest pytest-mock  # Already likely in dev deps\n```\n\n### Test Structure\n```\ntests/\n├── test_local_files.py          # Unit tests\n├── test_local_integration.py    # Integration tests\n└── fixtures/\n    └── sample_local.mp4         # Small test video\n```\n\n### Sample Test Cases\n```python\nimport pytest\nfrom claudetube.urls import is_local_file, generate_local_video_id\n\nclass TestLocalFileDetection:\n    def test_absolute_path(self):\n        assert is_local_file('/path/to/video.mp4') == True\n    \n    def test_relative_path(self):\n        assert is_local_file('./video.mp4') == True\n    \n    def test_home_path(self):\n        assert is_local_file('~/Videos/test.mp4') == True\n    \n    def test_file_uri(self):\n        assert is_local_file('file:///path/video.mp4') == True\n    \n    def test_http_url(self):\n        assert is_local_file('https://youtube.com/watch?v=abc') == False\n    \n    def test_nonexistent_file(self):\n        assert is_local_file('/nonexistent/video.mp4') == False\n\nclass TestVideoIdGeneration:\n    def test_deterministic(self):\n        id1 = generate_local_video_id('/path/to/video.mp4')\n        id2 = generate_local_video_id('/path/to/video.mp4')\n        assert id1 == id2\n    \n    def test_filesystem_safe(self):\n        video_id = generate_local_video_id('/path/with spaces/video!@#.mp4')\n        assert video_id.replace('_', '').replace('-', '').isalnum()\n```\n\n### Test Fixture: Small Sample Video\nOption A: Include in repo (if \u003c1MB)\nOption B: Download in CI setup step\nOption C: Generate programmatically with ffmpeg\n\n```python\n@pytest.fixture\ndef sample_video(tmp_path):\n    '''Generate a tiny test video with ffmpeg.'''\n    video = tmp_path / 'test.mp4'\n    subprocess.run([\n        'ffmpeg', '-f', 'lavfi', '-i', 'testsrc=duration=2:size=320x240',\n        '-f', 'lavfi', '-i', 'sine=frequency=440:duration=2',\n        str(video)\n    ])\n    return video\n```","status":"closed","priority":2,"issue_type":"chore","created_at":"2026-01-31T23:16:48.681985-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T10:45:52.888637-06:00","closed_at":"2026-02-01T10:45:52.888637-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-74k","depends_on_id":"claudetube-lk8","type":"blocks","created_at":"2026-01-31T23:17:14.204739-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-74k","depends_on_id":"claudetube-u4h","type":"parent-child","created_at":"2026-01-31T23:17:41.120573-06:00","created_by":"danielbarrett"}],"comments":[{"id":17,"issue_id":"claudetube-74k","author":"danielbarrett","text":"Commit: 5cf8b7f75a66db774a9bda828844e051ba53b59b","created_at":"2026-02-01T16:45:29Z"},{"id":18,"issue_id":"claudetube-74k","author":"danielbarrett","text":"## What was done\n- Created `tests/test_process_local_video.py` with 19 tests\n- Unit tests for `process_local_video()` function:\n  - Basic: returns VideoResult, success/error handling, video_id generation\n  - Caching: creates cache dir, saves state.json, symlink vs copy modes, cache hit\n  - Metadata: extracts via ffprobe, title from filename\n  - Transcription: creates transcript files, uses whisper_model param\n  - Path formats: absolute, relative, file:// URIs\n- Integration tests with real ffmpeg-generated video\n- All 369 tests pass (19 new + 350 existing)\n\nFiles:\n- tests/test_process_local_video.py (new, 19 tests)\n\n## Left undone\n- None - all acceptance criteria met\n\n## Gotchas\n- Integration tests generate 2-second test video with ffmpeg testsrc\n- Tests mock whisper to avoid slow transcription\n- pytest.mark.slow used for integration tests (custom mark warning)","created_at":"2026-02-01T16:45:44Z"}]}
{"id":"claudetube-8ey","title":"Fix ralph progress.txt tracking","description":"The scripts/ralph/progress.txt file is supposed to track iteration progress but the ralph-beads.sh script never writes to it. It only writes to ralph.log. Either remove progress.txt or update the script to append completed task summaries to it.","status":"closed","priority":3,"issue_type":"task","created_at":"2026-02-01T11:43:34.852288-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:44:40.145317-06:00","closed_at":"2026-02-01T11:44:40.145317-06:00","close_reason":"Done","comments":[{"id":54,"issue_id":"claudetube-8ey","author":"danielbarrett","text":"## What was done\n- Added PROGRESS_FILE variable to ralph-beads.sh\n- Script now appends task info to progress.txt when starting each task\n- Format: `- YYYY-MM-DD HH:MM | task-id [Pn] | title`\n\n## Left undone\n- None\n\n## Gotchas\n- Progress is logged when task starts, not completion (can't easily detect completion from output)","created_at":"2026-02-01T17:44:36Z"}]}
{"id":"claudetube-9f9","title":"Visual scene detection with PySceneDetect","description":"## User Story\nAs a fallback for videos without good transcript-based boundaries, I need visual scene detection using PySceneDetect.\n\n## Acceptance Criteria\n- [ ] Only triggered when cheap methods find \u003c3 boundaries in 5+ min video\n- [ ] Uses AdaptiveDetector for semantic (not just visual) cuts\n- [ ] Returns scene boundaries compatible with cheap detection format\n- [ ] Reasonable performance: \u003c2min for 30-min video\n\n## Technical Implementation\n\n### Library: scenedetect (PySceneDetect)\n```bash\npip install scenedetect[opencv]  # 300k+ downloads/month\n```\n\n```python\nfrom scenedetect import detect, AdaptiveDetector, ContentDetector\n\ndef detect_visual_boundaries(video_path: str) -\u003e list[dict]:\n    '''Fallback visual scene detection.'''\n    \n    # AdaptiveDetector is better for semantic boundaries\n    # ContentDetector is faster but catches more visual cuts\n    scenes = detect(video_path, AdaptiveDetector(\n        adaptive_threshold=3.0,\n        min_scene_len=30  # Minimum 30 frames (~1 second)\n    ))\n    \n    boundaries = []\n    for scene in scenes:\n        boundaries.append({\n            'timestamp': scene[0].get_seconds(),\n            'type': 'visual_scene',\n            'confidence': 0.75,\n            'sources': ['pyscenedetect']\n        })\n    \n    return boundaries\n```\n\n### When to Use Visual Detection\n```python\ndef should_use_visual_detection(\n    cheap_boundaries: list,\n    video_duration: float,\n    has_transcript: bool\n) -\u003e bool:\n    # Conditions for fallback\n    return (\n        len(cheap_boundaries) \u003c 3 and video_duration \u003e 300  # \u003c3 in 5+ min\n        or not has_transcript  # No transcript to analyze\n        or (video_duration / (len(cheap_boundaries) + 1)) \u003e 300  # Avg segment \u003e5 min\n    )\n```\n\n### Performance Optimization\n- Process at lower resolution: `video_manager.set_downscale_factor(2)`\n- Skip frames: `framerate_downscale=2`\n- Early termination if enough boundaries found\n\n### Alternative: ffmpeg scene detection\nIf PySceneDetect is too slow:\n```bash\nffmpeg -i video.mp4 -filter:v \"select='gt(scene,0.4)'\" -f null -\n```","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:18:44.408141-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:31:43.337779-06:00","closed_at":"2026-02-01T11:31:43.337779-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-9f9","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:35.390031-06:00","created_by":"danielbarrett"}],"comments":[{"id":33,"issue_id":"claudetube-9f9","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nVisual scene detection is the EXPENSIVE FALLBACK. It should ONLY run when:\n1. No YouTube chapters exist\n2. No description timestamps found\n3. Transcript analysis found \u003c5 boundaries for a 30+ min video\n\nBefore implementing, ensure the cheap detection chain (claudetube-3dt) is complete and provides insufficient coverage.\n\nTarget: This should run for \u003c20% of videos (those without chapters/timestamps).\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:11:43Z"},{"id":48,"issue_id":"claudetube-9f9","author":"danielbarrett","text":"## What was done\n- Added `scenedetect[opencv]\u003e=0.6.0` to pyproject.toml dependencies\n- Created `src/claudetube/analysis/visual.py` with:\n  - `should_use_visual_detection()` - determines when fallback is needed\n  - `detect_visual_boundaries()` - uses AdaptiveDetector (semantic boundaries)\n  - `detect_visual_boundaries_fast()` - uses ContentDetector (faster, less precise)\n- Exported new functions from `analysis/__init__.py`\n- Created comprehensive tests in `tests/test_visual.py` (22 tests, all passing)\n\nFiles: pyproject.toml, src/claudetube/analysis/__init__.py, src/claudetube/analysis/visual.py, tests/test_visual.py\n\n## Left undone\n- Integration with main processor (needs claudetube-vs1 smart segmentation strategy)\n\n## Gotchas\n- PySceneDetect's `detect()` helper function is nice but requires the scenedetect library to be fully installed\n- Better to use SceneManager directly for more control over detection parameters\n- AdaptiveDetector finds semantic cuts (topic changes); ContentDetector finds visual cuts (fast but noisy)\n- Downscale factor 2 provides good tradeoff between speed and accuracy","created_at":"2026-02-01T17:31:28Z"},{"id":49,"issue_id":"claudetube-9f9","author":"danielbarrett","text":"Commit: 87dbd6e8908a5b9c5f4ad79f1619c3d8060c95bc","created_at":"2026-02-01T17:31:36Z"}]}
{"id":"claudetube-9hk","title":"EPIC: Phase 4 - Progressive Learning","description":"The agent gets smarter about a video with each interaction. Core capabilities:\n- Interaction-driven enrichment (cache what Claude learns)\n- Multi-pass analysis (quick/standard/deep/exhaustive)\n- Cross-video learning and knowledge graph\n\nThis phase enables memory across sessions and connecting information across multiple videos.\n\n## Success Criteria\n- [ ] VideoMemory class stores learned facts per video\n- [ ] Multi-pass analysis depths available (quick/standard/deep)\n- [ ] Claude's answers cached and reused\n- [ ] Cross-video knowledge graph links related content\n- [ ] Subsequent queries benefit from prior analysis","status":"open","priority":3,"issue_type":"epic","created_at":"2026-01-31T23:18:05.637261-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T09:54:56.051616-06:00"}
{"id":"claudetube-9i5","title":"Implement ActiveVideoWatcher class","description":"## User Story\nAs an AI assistant, I want to actively decide what to examine in a video rather than passively analyzing everything.\n\n## Acceptance Criteria\n- [ ] Decides next action based on current understanding\n- [ ] Builds and updates hypotheses from findings\n- [ ] Stops when confidence is sufficient\n- [ ] Formulates answer with evidence\n\n## Technical Implementation\n\n### Core Watcher Class\n```python\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport numpy as np\n\n@dataclass\nclass Hypothesis:\n    claim: str\n    evidence: list[dict] = field(default_factory=list)\n    confidence: float = 0.0\n\n@dataclass\nclass WatcherAction:\n    action: str  # 'examine_quick', 'examine_deep', 'answer'\n    scene_id: Optional[int] = None\n    content: Optional[str] = None\n\nclass ActiveVideoWatcher:\n    '''Agent that actively decides what to examine.'''\n    \n    def __init__(self, video_id: str, user_goal: str, scenes: list[dict]):\n        self.video_id = video_id\n        self.user_goal = user_goal\n        self.scenes = scenes\n        self.examined = set()\n        self.hypotheses: list[Hypothesis] = []\n        self.confidence_threshold = 0.8\n        self.max_examinations = 10\n    \n    def decide_next_action(self) -\u003e WatcherAction:\n        '''Decide what to examine next.'''\n        \n        # Check if we have sufficient confidence\n        if self.has_sufficient_confidence():\n            return WatcherAction('answer', content=self.formulate_answer())\n        \n        # Check examination budget\n        if len(self.examined) \u003e= self.max_examinations:\n            return WatcherAction('answer', content=self.formulate_answer())\n        \n        # Rank unexplored scenes\n        candidates = self.rank_unexplored_scenes()\n        \n        if not candidates:\n            return WatcherAction('answer', content=self.formulate_answer())\n        \n        best = candidates[0]\n        \n        # Decide depth based on relevance\n        if best['relevance'] \u003e 0.8:\n            return WatcherAction('examine_deep', scene_id=best['scene_id'])\n        else:\n            return WatcherAction('examine_quick', scene_id=best['scene_id'])\n    \n    def rank_unexplored_scenes(self) -\u003e list[dict]:\n        '''Rank scenes by expected information gain.'''\n        candidates = []\n        \n        for scene in self.scenes:\n            if scene['segment_id'] in self.examined:\n                continue\n            \n            # Calculate relevance to goal\n            relevance = self.calculate_relevance(scene)\n            \n            candidates.append({\n                'scene_id': scene['segment_id'],\n                'relevance': relevance,\n                'scene': scene\n            })\n        \n        return sorted(candidates, key=lambda x: x['relevance'], reverse=True)\n    \n    def calculate_relevance(self, scene: dict) -\u003e float:\n        '''Estimate scene relevance to user goal.'''\n        # Use embedding similarity if available\n        if 'embedding' in scene:\n            goal_emb = embed_query(self.user_goal)\n            scene_emb = np.array(scene['embedding'])\n            return float(np.dot(goal_emb, scene_emb) / \n                        (np.linalg.norm(goal_emb) * np.linalg.norm(scene_emb)))\n        \n        # Fallback: keyword matching\n        transcript = scene.get('transcript_text', '').lower()\n        goal_words = set(self.user_goal.lower().split())\n        matches = sum(1 for w in goal_words if w in transcript)\n        return matches / len(goal_words) if goal_words else 0\n    \n    def update_understanding(self, scene_id: int, findings: list[dict]):\n        '''Update hypotheses based on examination findings.'''\n        self.examined.add(scene_id)\n        \n        for finding in findings:\n            matched = False\n            for hyp in self.hypotheses:\n                if self.finding_supports_hypothesis(finding, hyp):\n                    hyp.evidence.append(finding)\n                    hyp.confidence = self.calculate_confidence(hyp)\n                    matched = True\n                    break\n            \n            if not matched:\n                # New hypothesis\n                self.hypotheses.append(Hypothesis(\n                    claim=finding.get('claim', finding.get('description', '')),\n                    evidence=[finding],\n                    confidence=finding.get('initial_confidence', 0.3)\n                ))\n    \n    def has_sufficient_confidence(self) -\u003e bool:\n        if not self.hypotheses:\n            return False\n        return max(h.confidence for h in self.hypotheses) \u003e= self.confidence_threshold\n    \n    def formulate_answer(self) -\u003e dict:\n        '''Generate answer from hypotheses.'''\n        ranked = sorted(self.hypotheses, key=lambda h: h.confidence, reverse=True)\n        \n        if not ranked:\n            return {\n                'main_answer': 'Unable to determine from video content',\n                'confidence': 0,\n                'evidence': [],\n                'scenes_examined': len(self.examined)\n            }\n        \n        best = ranked[0]\n        return {\n            'main_answer': best.claim,\n            'confidence': best.confidence,\n            'evidence': [\n                {'timestamp': e.get('timestamp'), 'observation': e.get('description')}\n                for e in best.evidence\n            ],\n            'alternative_interpretations': [h.claim for h in ranked[1:3]],\n            'scenes_examined': len(self.examined)\n        }\n```","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-31T23:21:56.13209-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:32:35.219493-06:00","dependencies":[{"issue_id":"claudetube-9i5","depends_on_id":"claudetube-4wc","type":"parent-child","created_at":"2026-01-31T23:22:20.440352-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-9i5","depends_on_id":"claudetube-0oq","type":"blocks","created_at":"2026-01-31T23:22:20.87684-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-9i5","depends_on_id":"claudetube-i3x","type":"blocks","created_at":"2026-01-31T23:22:20.986411-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-asm","title":"Playlist metadata extraction","description":"## User Story\nAs a user learning from course playlists, I want playlist metadata extracted so I understand the video sequence and context.\n\n## Acceptance Criteria\n- [ ] Fetches playlist via yt-dlp\n- [ ] Extracts: playlist_id, title, description, channel\n- [ ] Lists all videos with position, title, duration\n- [ ] Infers playlist type (course, series, conference, collection)\n- [ ] Stores in playlists/{PLAYLIST_ID}/playlist.json\n\n## Technical Implementation\n\n### Library: yt-dlp (already a dependency)\n```python\nimport yt_dlp\nimport re\n\ndef extract_playlist_context(playlist_url: str) -\u003e dict:\n    '''Fetch playlist metadata without downloading videos.'''\n    \n    ydl_opts = {\n        'extract_flat': True,  # Don't download, just metadata\n        'quiet': True,\n    }\n    \n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        info = ydl.extract_info(playlist_url, download=False)\n    \n    return {\n        'playlist_id': info['id'],\n        'title': info['title'],\n        'description': info.get('description', ''),\n        'channel': info.get('channel', info.get('uploader', '')),\n        'video_count': len(info.get('entries', [])),\n        'videos': [\n            {\n                'video_id': entry['id'],\n                'title': entry['title'],\n                'duration': entry.get('duration'),\n                'position': idx,\n            }\n            for idx, entry in enumerate(info.get('entries', []))\n            if entry  # Skip unavailable videos\n        ],\n        'inferred_type': classify_playlist_type(info),\n    }\n```\n\n### Playlist Type Classification\n```python\ndef classify_playlist_type(playlist_info: dict) -\u003e str:\n    '''Infer playlist type from metadata patterns.'''\n    \n    title = playlist_info.get('title', '').lower()\n    video_titles = [v['title'].lower() for v in playlist_info.get('entries', []) if v]\n    \n    # Course detection\n    if any(s in title for s in ['course', 'tutorial', 'lesson', 'learn']):\n        return 'course'\n    \n    # Series detection (numbered episodes)\n    numbered = r'(part|ep|episode|#|chapter)\\s*\\d+'\n    if sum(1 for t in video_titles if re.search(numbered, t)) \u003e len(video_titles) * 0.5:\n        return 'series'\n    \n    # Conference detection\n    if any(s in title for s in ['conference', 'summit', 'meetup', 'talks', 'keynote']):\n        return 'conference'\n    \n    return 'collection'\n```\n\n### Cache Structure\n```\n~/.claude/video_cache/playlists/\n└── {PLAYLIST_ID}/\n    ├── playlist.json       # This metadata\n    └── videos/             # Symlinks (created by knowledge graph ticket)\n```","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:19:10.422475-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:47:39.103496-06:00","closed_at":"2026-02-01T11:47:39.103496-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-asm","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:36.013071-06:00","created_by":"danielbarrett"}],"comments":[{"id":56,"issue_id":"claudetube-asm","author":"danielbarrett","text":"## What was done\n- Created src/claudetube/operations/playlist.py\n- extract_playlist_metadata(): Fetches via yt-dlp flat-playlist mode\n- classify_playlist_type(): Detects course/series/conference/collection\n- save/load/list functions for playlist caching\n- Added tests/test_playlist.py\n\n## Left undone\n- MCP tool endpoint (follow-up ticket needed)\n- Symlinks to cached videos (per knowledge graph ticket)\n\n## Gotchas\n- yt-dlp flat-playlist outputs JSON lines (one per video + playlist header)\n- Playlist type detection uses 40% threshold for numbered videos","created_at":"2026-02-01T17:47:34Z"}]}
{"id":"claudetube-awk","title":"Implement attention priority modeling","description":"## User Story\nAs the watcher system, I need to model where attention should go (like a human expert would focus).\n\n## Acceptance Criteria\n- [ ] Scores scenes by multiple factors\n- [ ] Factors: relevance, density, novelty, visual salience, audio emphasis\n- [ ] Weights vary by video type\n- [ ] Returns priority score 0-1\n\n## Technical Implementation\n\n### Attention Priority Function\n```python\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass AttentionFactors:\n    relevance_to_goal: float\n    information_density: float\n    novelty: float\n    visual_salience: float\n    audio_emphasis: float\n    structural_importance: float\n\ndef calculate_attention_priority(\n    scene: dict,\n    user_goal: str,\n    video_type: str,\n    previous_scenes: list[dict]\n) -\u003e float:\n    '''Calculate how much attention a scene deserves.'''\n    \n    factors = AttentionFactors(\n        relevance_to_goal=calculate_relevance(scene, user_goal),\n        information_density=estimate_information_density(scene),\n        novelty=calculate_novelty(scene, previous_scenes),\n        visual_salience=detect_visual_salience(scene),\n        audio_emphasis=detect_audio_emphasis(scene),\n        structural_importance=get_structural_weight(scene, video_type)\n    )\n    \n    # Video-type-specific weights\n    weights = get_weights_for_video_type(video_type)\n    \n    priority = (\n        factors.relevance_to_goal * weights['relevance'] +\n        factors.information_density * weights['density'] +\n        factors.novelty * weights['novelty'] +\n        factors.visual_salience * weights['visual'] +\n        factors.audio_emphasis * weights['audio'] +\n        factors.structural_importance * weights['structure']\n    )\n    \n    return min(max(priority, 0), 1)  # Clamp to [0, 1]\n\ndef get_weights_for_video_type(video_type: str) -\u003e dict:\n    '''Return attention weights based on video type.'''\n    \n    weights = {\n        'coding_tutorial': {\n            'relevance': 0.30, 'density': 0.25, 'visual': 0.25,\n            'novelty': 0.10, 'audio': 0.05, 'structure': 0.05\n        },\n        'lecture': {\n            'relevance': 0.25, 'audio': 0.25, 'structure': 0.20,\n            'novelty': 0.15, 'density': 0.10, 'visual': 0.05\n        },\n        'demo': {\n            'relevance': 0.30, 'visual': 0.30, 'novelty': 0.20,\n            'density': 0.10, 'audio': 0.05, 'structure': 0.05\n        },\n        'interview': {\n            'relevance': 0.30, 'audio': 0.30, 'structure': 0.15,\n            'novelty': 0.15, 'density': 0.05, 'visual': 0.05\n        }\n    }\n    \n    return weights.get(video_type, {\n        'relevance': 0.20, 'density': 0.15, 'visual': 0.15,\n        'novelty': 0.15, 'audio': 0.15, 'structure': 0.15\n    })\n```\n\n### Factor Calculations\n```python\ndef estimate_information_density(scene: dict) -\u003e float:\n    '''Estimate how much information is in a scene.'''\n    transcript_len = len(scene.get('transcript_text', ''))\n    ocr_items = len(scene.get('technical', {}).get('ocr_text', []))\n    code_blocks = len(scene.get('technical', {}).get('code_blocks', []))\n    \n    # Normalize (higher is denser)\n    word_density = min(transcript_len / 500, 1.0)  # 500 words = max\n    visual_density = min((ocr_items + code_blocks * 3) / 10, 1.0)\n    \n    return (word_density + visual_density) / 2\n\ndef calculate_novelty(scene: dict, previous: list[dict]) -\u003e float:\n    '''Calculate how different this scene is from previous.'''\n    if not previous or 'embedding' not in scene:\n        return 0.5\n    \n    scene_emb = np.array(scene['embedding'])\n    prev_embs = [np.array(s['embedding']) for s in previous if 'embedding' in s]\n    \n    if not prev_embs:\n        return 0.5\n    \n    # Average similarity to previous scenes\n    similarities = [\n        np.dot(scene_emb, p) / (np.linalg.norm(scene_emb) * np.linalg.norm(p))\n        for p in prev_embs\n    ]\n    avg_similarity = np.mean(similarities)\n    \n    return 1 - avg_similarity  # Novelty = inverse of similarity\n\ndef detect_visual_salience(scene: dict) -\u003e float:\n    '''Detect if scene has visually important content.'''\n    technical = scene.get('technical', {})\n    content_type = technical.get('content_type', 'unknown')\n    \n    # Code/diagrams are highly salient\n    if content_type in ['code', 'diagram', 'slides']:\n        return 0.8\n    elif content_type == 'terminal':\n        return 0.6\n    elif technical.get('ocr_text'):\n        return 0.5\n    else:\n        return 0.2\n\ndef detect_audio_emphasis(scene: dict) -\u003e float:\n    '''Detect if speaker emphasizes this content.'''\n    transcript = scene.get('transcript_text', '').lower()\n    \n    emphasis_phrases = [\n        'important', 'key point', 'remember', 'crucial', 'essential',\n        'pay attention', 'note that', 'critical', 'the main'\n    ]\n    \n    matches = sum(1 for phrase in emphasis_phrases if phrase in transcript)\n    return min(matches * 0.2, 1.0)\n```","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-31T23:22:00.381194-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:32:54.234973-06:00","dependencies":[{"issue_id":"claudetube-awk","depends_on_id":"claudetube-4wc","type":"parent-child","created_at":"2026-01-31T23:22:20.552577-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-awk","depends_on_id":"claudetube-kvk","type":"blocks","created_at":"2026-01-31T23:22:21.095223-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-bdj","title":"Add /yt:watch command for active viewing","description":"## User Story\nAs a Claude Code user, I want a /yt:watch command that actively explores and reasons about videos.\n\n## Acceptance Criteria\n- [ ] Uses ActiveVideoWatcher for exploration\n- [ ] Returns answer with confidence + evidence\n- [ ] Shows scenes examined and reasoning\n- [ ] This is the culmination: human-like video comprehension\n\n## Technical Implementation\n\n### MCP Tool\n```python\n@mcp.tool()\ndef watch_video_tool(video_id: str, question: str) -\u003e str:\n    '''Actively watch and reason about a video to answer a question.\n    \n    Uses an intelligent watching strategy that:\n    1. Identifies most relevant scenes\n    2. Examines them progressively\n    3. Builds hypotheses and gathers evidence\n    4. Returns answer with confidence and evidence\n    \n    This is the most thorough analysis mode.\n    '''\n    \n    cache_dir = CACHE_BASE / video_id\n    \n    # Load video data\n    scenes = load_scenes(cache_dir)\n    if not scenes:\n        return json.dumps({'error': 'Video not processed. Run process_video first.'})\n    \n    # Create active watcher\n    watcher = ActiveVideoWatcher(video_id, question, scenes)\n    \n    # Active exploration loop\n    max_iterations = 15\n    examination_log = []\n    \n    for i in range(max_iterations):\n        action = watcher.decide_next_action()\n        \n        if action.action == 'answer':\n            break\n        \n        # Execute action\n        if action.action in ['examine_quick', 'examine_deep']:\n            scene = next(s for s in scenes if s['segment_id'] == action.scene_id)\n            \n            # Examine scene\n            if action.action == 'examine_deep':\n                findings = examine_scene_deep(scene, question)\n            else:\n                findings = examine_scene_quick(scene, question)\n            \n            # Update understanding\n            watcher.update_understanding(action.scene_id, findings)\n            \n            examination_log.append({\n                'iteration': i,\n                'scene_id': action.scene_id,\n                'depth': action.action,\n                'findings_count': len(findings)\n            })\n    \n    # Formulate final answer\n    answer = watcher.formulate_answer()\n    \n    # Verify comprehension\n    verification = verify_comprehension({\n        'scenes': scenes,\n        'answer': answer\n    })\n    \n    result = {\n        'video_id': video_id,\n        'question': question,\n        'answer': answer['main_answer'],\n        'confidence': answer['confidence'],\n        'evidence': answer['evidence'],\n        'alternative_interpretations': answer.get('alternative_interpretations', []),\n        'examination_log': examination_log,\n        'scenes_examined': answer['scenes_examined'],\n        'comprehension_verified': verification['ready_to_answer']\n    }\n    \n    return json.dumps(result, indent=2)\n```\n\n### Examination Functions\n```python\ndef examine_scene_quick(scene: dict, question: str) -\u003e list[dict]:\n    '''Quick examination: transcript + existing visual description.'''\n    findings = []\n    \n    transcript = scene.get('transcript_text', '')\n    if question_relevant_to_text(question, transcript):\n        findings.append({\n            'type': 'transcript_match',\n            'description': f'Transcript mentions relevant content',\n            'timestamp': scene['start'],\n            'initial_confidence': 0.5\n        })\n    \n    visual = scene.get('visual', {}).get('description', '')\n    if visual and question_relevant_to_text(question, visual):\n        findings.append({\n            'type': 'visual_match',\n            'description': f'Visual content relevant',\n            'timestamp': scene['start'],\n            'initial_confidence': 0.6\n        })\n    \n    return findings\n\ndef examine_scene_deep(scene: dict, question: str) -\u003e list[dict]:\n    '''Deep examination: extract frames, analyze with vision model.'''\n    \n    # Get keyframes\n    frames = extract_keyframes_for_scene(scene)\n    \n    # Analyze with vision model\n    analysis = analyze_frames_for_question(frames, question)\n    \n    findings = []\n    for item in analysis:\n        findings.append({\n            'type': 'deep_analysis',\n            'description': item['description'],\n            'claim': item.get('claim'),\n            'timestamp': scene['start'],\n            'initial_confidence': item.get('confidence', 0.7)\n        })\n    \n    return findings\n```\n\n### Example Output\n```json\n{\n  \"video_id\": \"abc123\",\n  \"question\": \"What bug was fixed and how?\",\n  \"answer\": \"An off-by-one error in the authentication loop was fixed by changing the comparison from \u003c to \u003c= on line 42\",\n  \"confidence\": 0.85,\n  \"evidence\": [\n    {\"timestamp\": 245.2, \"observation\": \"Code shows loop with \u003c operator\"},\n    {\"timestamp\": 312.5, \"observation\": \"Fix applied: changed to \u003c=, test passes\"}\n  ],\n  \"alternative_interpretations\": [\n    \"Could also be a race condition fix mentioned earlier\"\n  ],\n  \"scenes_examined\": 5,\n  \"comprehension_verified\": true\n}\n```","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-31T23:22:08.748048-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:33:27.358172-06:00","dependencies":[{"issue_id":"claudetube-bdj","depends_on_id":"claudetube-4wc","type":"parent-child","created_at":"2026-01-31T23:22:20.772107-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-bdj","depends_on_id":"claudetube-9i5","type":"blocks","created_at":"2026-01-31T23:22:21.312896-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-bdj","depends_on_id":"claudetube-awk","type":"blocks","created_at":"2026-01-31T23:22:21.418251-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-bv5","title":"Track objects and concepts across scenes","description":"## User Story\nAs a user, I want to track objects and concepts mentioned throughout the video.\n\n## Acceptance Criteria\n- [ ] Tracks objects appearing/disappearing visually\n- [ ] Tracks concepts mentioned in transcript\n- [ ] Builds entity timeline\n- [ ] Stores in entities/objects.json and entities/concepts.json\n\n## Technical Implementation\n\n### Object Tracking from Visual Descriptions\n```python\nfrom collections import defaultdict\n\ndef track_objects(scenes: list[dict]) -\u003e dict:\n    '''Track physical objects across scenes.'''\n    \n    objects = defaultdict(list)\n    \n    for scene in scenes:\n        visual = scene.get('visual', {})\n        detected = visual.get('objects', [])\n        \n        for obj in detected:\n            objects[obj.lower()].append({\n                'scene_id': scene['segment_id'],\n                'timestamp': scene['start'],\n            })\n    \n    return {\n        obj: {\n            'appearances': appearances,\n            'first_seen': appearances[0]['timestamp'],\n            'last_seen': appearances[-1]['timestamp'],\n            'frequency': len(appearances)\n        }\n        for obj, appearances in objects.items()\n    }\n```\n\n### Concept Tracking from Transcript\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\n\ndef track_concepts(scenes: list[dict], top_n: int = 20) -\u003e dict:\n    '''Extract and track key concepts from transcript.'''\n    \n    # Extract keywords per scene\n    texts = [s.get('transcript_text', '') for s in scenes]\n    \n    vectorizer = TfidfVectorizer(\n        stop_words='english',\n        ngram_range=(1, 2),\n        max_features=100\n    )\n    tfidf = vectorizer.fit_transform(texts)\n    feature_names = vectorizer.get_feature_names_out()\n    \n    # Track each concept\n    concepts = defaultdict(list)\n    \n    for scene_idx, scene in enumerate(scenes):\n        # Get top terms for this scene\n        scene_tfidf = tfidf[scene_idx].toarray()[0]\n        top_indices = scene_tfidf.argsort()[-5:][::-1]\n        \n        for idx in top_indices:\n            if scene_tfidf[idx] \u003e 0.1:\n                term = feature_names[idx]\n                concepts[term].append({\n                    'scene_id': scene['segment_id'],\n                    'timestamp': scene['start'],\n                    'score': float(scene_tfidf[idx])\n                })\n    \n    # Filter to top_n most frequent\n    sorted_concepts = sorted(\n        concepts.items(),\n        key=lambda x: len(x[1]),\n        reverse=True\n    )[:top_n]\n    \n    return {\n        concept: {\n            'mentions': mentions,\n            'first_mention': mentions[0]['timestamp'],\n            'frequency': len(mentions)\n        }\n        for concept, mentions in sorted_concepts\n    }\n```\n\n### Output Format\n```json\n{\n  \"objects\": {\n    \"laptop\": {\"appearances\": [...], \"frequency\": 12},\n    \"whiteboard\": {\"appearances\": [...], \"frequency\": 5}\n  },\n  \"concepts\": {\n    \"authentication\": {\"mentions\": [...], \"frequency\": 8},\n    \"jwt token\": {\"mentions\": [...], \"frequency\": 6}\n  }\n}\n```","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-31T23:20:52.904384-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:30:59.823473-06:00","dependencies":[{"issue_id":"claudetube-bv5","depends_on_id":"claudetube-sa5","type":"parent-child","created_at":"2026-01-31T23:21:04.841593-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-c5q","title":"Add MCP tool for playlist metadata","description":"Follow-up to claudetube-asm. Add process_playlist_tool to mcp_server.py that exposes extract_playlist_metadata().","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-01T11:49:19.255378-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:50:13.680121-06:00","closed_at":"2026-02-01T11:50:13.680121-06:00","close_reason":"Done","comments":[{"id":58,"issue_id":"claudetube-c5q","author":"danielbarrett","text":"## What was done\n- Added get_playlist() MCP tool to extract and cache playlist metadata\n- Added list_playlists() MCP tool to list cached playlists\n- Both tools use the playlist.py module from claudetube-asm\n\n## Left undone\n- None\n\n## Gotchas\n- Playlist cache is in playlists/{id}/ subdirectory, separate from videos","created_at":"2026-02-01T17:50:08Z"}]}
{"id":"claudetube-did","title":"Implement comprehension verification","description":"## User Story\nAs the system, I want to verify that understanding is actually correct before answering.\n\n## Acceptance Criteria\n- [ ] Generates self-test questions\n- [ ] Answers from understanding alone (no re-examining)\n- [ ] Verifies answers against video content\n- [ ] Returns comprehension score and gaps\n\n## Technical Implementation\n\n### Comprehension Verifier\n```python\nfrom dataclasses import dataclass\n\n@dataclass\nclass VerificationResult:\n    question: str\n    answer: str\n    verified: bool\n    confidence: float\n\ndef verify_comprehension(\n    video_understanding: dict,\n    verification_questions: list[str] = None\n) -\u003e dict:\n    '''Verify the agent's understanding of the video.'''\n    \n    if verification_questions is None:\n        verification_questions = generate_self_test_questions(video_understanding)\n    \n    results = []\n    for question in verification_questions:\n        # Answer from understanding only\n        answer = answer_from_understanding(video_understanding, question)\n        \n        # Verify against video content\n        verification = verify_answer(answer, question, video_understanding)\n        \n        results.append(VerificationResult(\n            question=question,\n            answer=answer,\n            verified=verification['correct'],\n            confidence=verification['confidence']\n        ))\n    \n    # Calculate overall score\n    comprehension_score = sum(r.verified for r in results) / len(results)\n    \n    return {\n        'score': comprehension_score,\n        'results': [vars(r) for r in results],\n        'gaps': [r.question for r in results if not r.verified],\n        'ready_to_answer': comprehension_score \u003e= 0.7\n    }\n```\n\n### Question Generation\n```python\ndef generate_self_test_questions(understanding: dict) -\u003e list[str]:\n    '''Generate questions to test understanding.'''\n    \n    scenes = understanding.get('scenes', [])\n    structure = understanding.get('structure', {})\n    \n    questions = []\n    \n    # Basic comprehension\n    questions.append('What is the main topic of this video?')\n    \n    # Section-specific\n    if structure.get('sections'):\n        random_section = random.choice(structure['sections'])\n        questions.append(f\"What is covered in the section starting at {format_timestamp(random_section['start'])}?\")\n    \n    # Content-specific\n    if scenes:\n        random_scene = random.choice(scenes)\n        questions.append(f\"What is happening at {format_timestamp(random_scene['start'])}?\")\n    \n    # Synthesis\n    questions.append('What would someone learn from watching this video?')\n    \n    return questions\n\ndef answer_from_understanding(understanding: dict, question: str) -\u003e str:\n    '''Answer question using only cached understanding.'''\n    \n    # Search through understanding for relevant info\n    scenes = understanding.get('scenes', [])\n    structure = understanding.get('structure', {})\n    memory = understanding.get('memory', {})\n    \n    # Check if we've answered similar before\n    qa_history = memory.get('qa_history', [])\n    for qa in qa_history:\n        if similar_question(question, qa['question']):\n            return qa['answer']\n    \n    # Synthesize from scene summaries\n    relevant_text = []\n    for scene in scenes:\n        if scene_relevant_to_question(scene, question):\n            relevant_text.append(scene.get('transcript_text', '')[:200])\n    \n    if relevant_text:\n        return f'Based on the video content: {\" \".join(relevant_text[:3])}'\n    \n    return 'Unable to answer from current understanding'\n\ndef verify_answer(answer: str, question: str, understanding: dict) -\u003e dict:\n    '''Check if answer is supported by video content.'''\n    \n    # Simple verification: check if answer terms appear in transcripts\n    scenes = understanding.get('scenes', [])\n    all_text = ' '.join(s.get('transcript_text', '') for s in scenes).lower()\n    \n    answer_words = set(answer.lower().split())\n    important_words = {w for w in answer_words if len(w) \u003e 4}\n    \n    if not important_words:\n        return {'correct': False, 'confidence': 0.0}\n    \n    matches = sum(1 for w in important_words if w in all_text)\n    confidence = matches / len(important_words)\n    \n    return {\n        'correct': confidence \u003e 0.5,\n        'confidence': confidence\n    }\n```","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-31T23:22:04.587515-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:33:08.011933-06:00","dependencies":[{"issue_id":"claudetube-did","depends_on_id":"claudetube-4wc","type":"parent-child","created_at":"2026-01-31T23:22:20.664731-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-did","depends_on_id":"claudetube-9i5","type":"blocks","created_at":"2026-01-31T23:22:21.204822-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-dlk","title":"Update CacheManager to use configurable cache directory","description":"## User Story\nAs a developer, I need CacheManager to respect the configured cache directory.\n\n## Acceptance Criteria\n- [ ] CacheManager uses get_cache_dir() instead of hardcoded CACHE_DIR\n- [ ] Existing code continues to work (backwards compatible)\n- [ ] Cache directory is created if it doesn't exist\n\n## Files to modify\n- src/claudetube/cache/manager.py\n- src/claudetube/config/defaults.py","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-01T10:03:57.919144-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T10:58:13.997381-06:00","closed_at":"2026-02-01T10:58:13.997381-06:00","close_reason":"CacheManager updated to use config loader","dependencies":[{"issue_id":"claudetube-dlk","depends_on_id":"claudetube-quz","type":"blocks","created_at":"2026-02-01T10:29:43.771345-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-dlk","depends_on_id":"claudetube-dug","type":"parent-child","created_at":"2026-02-01T10:30:37.43623-06:00","created_by":"danielbarrett"}],"comments":[{"id":22,"issue_id":"claudetube-dlk","author":"danielbarrett","text":"Commit: f0cc27de0449333e3a2e180766aea96337e24726\n\n## What was done\n- CacheManager now uses get_cache_dir() instead of hardcoded CACHE_DIR\n- Updated config loader with path resolution and ensure_exists option\n- All operations modules use configurable cache through CacheManager\n\nFiles: cache/manager.py, config/loader.py, config/defaults.py, config/__init__.py, operations/*\n\n## Left undone\n- None\n\n## Gotchas\n- get_cache_dir(ensure_exists=True) creates directory automatically","created_at":"2026-02-01T16:58:13Z"}]}
{"id":"claudetube-dth","title":"EPIC: Phase 2 - Semantic Search \u0026 Retrieval","description":"Enable 'find the part where...' queries without scanning entire video. Core capabilities:\n- Multimodal scene embeddings (visual + audio + text)\n- Vector index with ChromaDB/FAISS\n- Temporal grounding tool (/yt:find)\n- Natural language moment search\n\nThis phase enables Claude to jump directly to relevant sections instead of guessing timestamps.\n\n## Success Criteria\n- [ ] Scenes have vector embeddings (text + visual)\n- [ ] Vector index persisted in cache\n- [ ] /yt:find 'query' returns ranked timestamps\n- [ ] Natural language queries work (e.g., 'when does he show the database schema')\n- [ ] Search results include confidence scores","status":"open","priority":2,"issue_type":"epic","created_at":"2026-01-31T23:17:57.356873-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T09:54:48.526996-06:00","comments":[{"id":30,"issue_id":"claudetube-dth","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nSemantic search MUST follow the fallback hierarchy:\n1. **CACHE** - Embeddings already computed? Use them.\n2. **TRANSCRIPT** - Text search is cheap, try it first.\n3. **EMBEDDINGS** - Compute only for scenes that need them.\n4. **VISUAL** - Multimodal embeddings only when text fails.\n\nIndex incrementally. Don't recompute embeddings for cached scenes.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:11:23Z"}]}
{"id":"claudetube-dug","title":"EPIC: Configurable Cache/Data Directory","description":"## Summary\nAllow users to configure where claudetube stores its cache/data directory instead of hardcoding ~/.claude/video_cache.\n\n## Motivation\nUsers may want to:\n- Store cache in the project repo (.claudetube/)\n- Use a shared/mounted drive for caching\n- Keep cache separate from ~/.claude for organization\n- Use environment variables for CI/CD pipelines\n\n## Configuration Priority (highest to lowest)\n1. Environment variable: CLAUDETUBE_CACHE_DIR\n2. Project config: .claudetube/config.yaml or pyproject.toml [tool.claudetube]\n3. User config: ~/.config/claudetube/config.yaml\n4. Default: ~/.claude/video_cache\n\n## Success Criteria\n- [ ] Cache directory is configurable via environment variable\n- [ ] Cache directory is configurable via project config file\n- [ ] Cache directory is configurable via user config file\n- [ ] Default behavior unchanged (backwards compatible)\n- [ ] Config is persisted and remembered across sessions\n- [ ] MCP server respects configuration\n- [ ] Documentation updated with configuration options","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-02-01T10:03:30.448995-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:49:13.926493-06:00","closed_at":"2026-02-01T11:49:13.926493-06:00","close_reason":"All 7 child tickets completed: ENV var, project config, user config, config loader, cache manager, MCP server integration, and documentation.","comments":[{"id":40,"issue_id":"claudetube-dug","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nConfigurable cache is CRITICAL to the architecture. The cache enables:\n1. Instant returns for already-processed content\n2. No duplicate compute for the same video\n3. Incremental processing (add scenes without reprocessing transcript)\n\nCache location must be fast (local SSD preferred). Network mounts will hurt latency.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:12:22Z"}]}
{"id":"claudetube-fcp","title":"Support project-level .claudetube/config.yaml","description":"## User Story\nAs a user, I want to create a .claudetube/config.yaml in my project to configure claudetube for that project.\n\n## Acceptance Criteria\n- [ ] Look for .claudetube/config.yaml in current dir and parent dirs\n- [ ] Parse YAML config file\n- [ ] Support cache_dir key for cache location\n- [ ] Relative paths resolve relative to config file location\n- [ ] Config is optional (no error if missing)\n\n## Config Format\n```yaml\n# .claudetube/config.yaml\ncache_dir: .claudetube/cache  # Relative to this file\n# or\ncache_dir: /absolute/path/to/cache\n```\n\n## Files to modify\n- src/claudetube/config/defaults.py (or new config/loader.py)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-01T10:03:45.942787-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:42:47.120179-06:00","closed_at":"2026-02-01T11:42:47.120179-06:00","close_reason":"Implemented with 9 passing tests","dependencies":[{"issue_id":"claudetube-fcp","depends_on_id":"claudetube-quz","type":"blocks","created_at":"2026-02-01T10:29:43.535919-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-fcp","depends_on_id":"claudetube-dug","type":"parent-child","created_at":"2026-02-01T10:30:37.061551-06:00","created_by":"danielbarrett"}],"comments":[{"id":51,"issue_id":"claudetube-fcp","author":"danielbarrett","text":"Commit: fd8f243333da1fc760eb6a7524959f36b25a4cd6\n\n## What was done\n- Fixed relative path resolution to be relative to config file location (not cwd)\n- Added 9 comprehensive tests for project config functionality\n- Tests cover: config discovery, relative/absolute paths, tilde expansion, priority, fallthrough\n\nFiles: src/claudetube/config/loader.py, tests/test_config_project.py\n\n## Left undone\n- None\n\n## Gotchas\n- Relative paths like './cache' resolve relative to .claudetube/ directory, not cwd","created_at":"2026-02-01T17:42:37Z"}]}
{"id":"claudetube-h5b","title":"Cross-video knowledge graph","description":"## User Story\nAs a user who watches many related videos, I want connections tracked across videos.\n\n## Acceptance Criteria\n- [ ] Indexes video concepts into global graph\n- [ ] Finds videos related to a topic\n- [ ] Links entities across videos\n- [ ] Stores in ~/.claude/video_knowledge/graph.json\n\n## Technical Implementation\n\n### Library: Just JSON (simpler) or networkx (if graph queries needed)\n```bash\npip install networkx  # Optional, for complex graph queries\n```\n\n### Simple JSON Implementation\n```python\nimport json\nfrom pathlib import Path\nfrom collections import defaultdict\nfrom datetime import datetime\n\nclass VideoKnowledgeGraph:\n    '''Track concepts and entities across all videos.'''\n    \n    def __init__(self):\n        self.graph_path = Path.home() / '.claude' / 'video_knowledge' / 'graph.json'\n        self.graph_path.parent.mkdir(parents=True, exist_ok=True)\n        self.graph = self._load()\n    \n    def _load(self) -\u003e dict:\n        if self.graph_path.exists():\n            return json.loads(self.graph_path.read_text())\n        return {'videos': {}, 'entities': {}, 'concepts': {}}\n    \n    def _save(self):\n        self.graph_path.write_text(json.dumps(self.graph, indent=2))\n    \n    def add_video(self, video_id: str, metadata: dict, entities: dict, concepts: list[str]):\n        '''Index a video into the knowledge graph.'''\n        \n        # Add video node\n        self.graph['videos'][video_id] = {\n            'title': metadata.get('title', ''),\n            'channel': metadata.get('channel', ''),\n            'indexed_at': datetime.now().isoformat()\n        }\n        \n        # Link entities\n        for entity_type, entity_list in entities.items():\n            for entity in entity_list:\n                entity_key = entity.lower().strip()\n                if entity_key not in self.graph['entities']:\n                    self.graph['entities'][entity_key] = {\n                        'type': entity_type,\n                        'videos': []\n                    }\n                if video_id not in self.graph['entities'][entity_key]['videos']:\n                    self.graph['entities'][entity_key]['videos'].append(video_id)\n        \n        # Link concepts\n        for concept in concepts:\n            concept_key = concept.lower().strip()\n            if concept_key not in self.graph['concepts']:\n                self.graph['concepts'][concept_key] = {'videos': []}\n            if video_id not in self.graph['concepts'][concept_key]['videos']:\n                self.graph['concepts'][concept_key]['videos'].append(video_id)\n        \n        self._save()\n    \n    def find_related_videos(self, query: str) -\u003e list[dict]:\n        '''Find videos related to a concept or entity.'''\n        query_lower = query.lower()\n        matches = []\n        \n        # Search entities\n        for entity_key, entity_data in self.graph['entities'].items():\n            if query_lower in entity_key:\n                for video_id in entity_data['videos']:\n                    matches.append({\n                        'video_id': video_id,\n                        'match_type': 'entity',\n                        'matched': entity_key,\n                        'video_title': self.graph['videos'].get(video_id, {}).get('title', '')\n                    })\n        \n        # Search concepts\n        for concept_key, concept_data in self.graph['concepts'].items():\n            if query_lower in concept_key:\n                for video_id in concept_data['videos']:\n                    matches.append({\n                        'video_id': video_id,\n                        'match_type': 'concept',\n                        'matched': concept_key,\n                        'video_title': self.graph['videos'].get(video_id, {}).get('title', '')\n                    })\n        \n        # Deduplicate\n        seen = set()\n        unique = []\n        for m in matches:\n            if m['video_id'] not in seen:\n                seen.add(m['video_id'])\n                unique.append(m)\n        \n        return unique\n    \n    def get_video_connections(self, video_id: str) -\u003e list[str]:\n        '''Get other videos sharing entities/concepts with this one.'''\n        my_entities = set()\n        my_concepts = set()\n        \n        for key, data in self.graph['entities'].items():\n            if video_id in data['videos']:\n                my_entities.add(key)\n        \n        for key, data in self.graph['concepts'].items():\n            if video_id in data['videos']:\n                my_concepts.add(key)\n        \n        connected = set()\n        for key in my_entities | my_concepts:\n            source = self.graph['entities'] if key in self.graph['entities'] else self.graph['concepts']\n            for vid in source.get(key, {}).get('videos', []):\n                if vid \\!= video_id:\n                    connected.add(vid)\n        \n        return list(connected)\n```\n\n### MCP Tool\n```python\n@mcp.tool()\ndef find_related_videos_tool(query: str) -\u003e str:\n    '''Find videos related to a topic across all cached videos.'''\n    graph = VideoKnowledgeGraph()\n    matches = graph.find_related_videos(query)\n    return json.dumps({'query': query, 'matches': matches}, indent=2)\n```","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-31T23:21:24.350432-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:31:54.900717-06:00","dependencies":[{"issue_id":"claudetube-h5b","depends_on_id":"claudetube-9hk","type":"parent-child","created_at":"2026-01-31T23:21:45.715296-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-h5b","depends_on_id":"claudetube-uzo","type":"blocks","created_at":"2026-01-31T23:21:46.141914-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-hnz","title":"Update cache structure for scenes","description":"## User Story\nAs a developer, I need a well-defined cache structure for all the new scene data.\n\n## Acceptance Criteria\n- [ ] scenes/ directory with scenes.json\n- [ ] Per-scene subdirectories with keyframes, visual.json, technical.json\n- [ ] state.json tracks scene processing status\n- [ ] Backward compatible (existing caches still work)\n\n## Technical Implementation\n\n### Directory Structure\n```\n~/.claude/video_cache/{VIDEO_ID}/\n├── state.json              # Existing - add scenes_processed flag\n├── audio.mp3               # Existing\n├── audio.srt               # Existing\n├── audio.txt               # Existing\n├── thumbnail.jpg           # Existing\n│\n├── scenes/\n│   ├── scenes.json         # Scene boundaries and metadata\n│   ├── scene_000/\n│   │   ├── keyframes/\n│   │   │   ├── kf_000.jpg\n│   │   │   ├── kf_001.jpg\n│   │   │   └── kf_002.jpg\n│   │   ├── visual.json     # Visual transcript\n│   │   └── technical.json  # OCR, code extraction\n│   ├── scene_001/\n│   │   └── ...\n│   └── ...\n│\n├── drill_lowest/           # Existing frame extraction\n├── drill_low/\n├── hq/\n│\n└── memory/                 # Phase 4 - agent memory\n    ├── observations.json\n    └── qa_history.json\n```\n\n### state.json Additions\n```json\n{\n  \"video_id\": \"abc123\",\n  \"transcript_complete\": true,\n  \"scenes_processed\": true,\n  \"scenes_method\": \"transcript\",\n  \"scene_count\": 12,\n  \"visual_transcripts_complete\": false,\n  \"technical_extraction_complete\": false\n}\n```\n\n### Helper Functions\n```python\nfrom pathlib import Path\n\ndef get_scene_dir(cache_dir: Path, scene_id: int) -\u003e Path:\n    scene_dir = cache_dir / 'scenes' / f'scene_{scene_id:03d}'\n    scene_dir.mkdir(parents=True, exist_ok=True)\n    return scene_dir\n\ndef get_keyframes_dir(cache_dir: Path, scene_id: int) -\u003e Path:\n    kf_dir = get_scene_dir(cache_dir, scene_id) / 'keyframes'\n    kf_dir.mkdir(parents=True, exist_ok=True)\n    return kf_dir\n\ndef is_scene_processed(cache_dir: Path, scene_id: int) -\u003e dict:\n    scene_dir = get_scene_dir(cache_dir, scene_id)\n    return {\n        'keyframes': (scene_dir / 'keyframes').exists(),\n        'visual': (scene_dir / 'visual.json').exists(),\n        'technical': (scene_dir / 'technical.json').exists(),\n    }\n```\n\n### Backward Compatibility\n- Check for scenes/ dir before accessing\n- Segmentation creates scenes/ on first run\n- Existing drill/ and hq/ dirs unaffected","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:19:23.638802-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T09:55:10.100674-06:00","closed_at":"2026-02-01T09:55:10.100674-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-hnz","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:36.345551-06:00","created_by":"danielbarrett"}],"comments":[{"id":9,"issue_id":"claudetube-hnz","author":"danielbarrett","text":"Commit: 3389ec65c729813cee49965a5298f65b6635fe42","created_at":"2026-02-01T15:54:39Z"},{"id":10,"issue_id":"claudetube-hnz","author":"danielbarrett","text":"## What was done\n- Added scene-related fields to VideoState: scenes_processed, scenes_method, scene_count, visual_transcripts_complete, technical_extraction_complete\n- Created cache/scenes.py with SceneBoundary, ScenesData, and SceneStatus dataclasses\n- Implemented helper functions: get_scenes_dir, get_scene_dir, get_keyframes_dir, get_scenes_json_path, get_visual_json_path, get_technical_json_path, has_scenes, load_scenes_data, save_scenes_data, get_scene_status, list_scene_keyframes, get_all_scene_statuses\n- Added convenience methods to CacheManager for all scene operations\n- Created test_scenes.py with 44 comprehensive tests\n- Files: src/claudetube/cache/scenes.py, src/claudetube/cache/__init__.py, src/claudetube/cache/manager.py, src/claudetube/models/state.py, tests/test_scenes.py\n\n## Left undone\n- None\n\n## Gotchas\n- get_scenes_dir and similar functions create directories as a side effect (mkdir with parents=True), which required exist_ok=True in one test","created_at":"2026-02-01T15:55:02Z"}]}
{"id":"claudetube-hqq","title":"Extract audio from local video for whisper","description":"## User Story\nAs a user with local video files, I need audio extracted for whisper transcription using ffmpeg directly (not yt-dlp).\n\n## Acceptance Criteria\n- [ ] Extracts audio to audio.mp3 in cache directory\n- [ ] Handles video files (extract audio track)\n- [ ] Handles audio-only files (convert/copy)\n- [ ] Skips extraction if audio.mp3 already exists\n- [ ] Works with common codecs (h264, h265, vp9, av1)\n- [ ] Reasonable quality for speech recognition (128kbps sufficient)\n\n## Technical Implementation\n\n### Library: ffmpeg-python\nAlready a dependency from metadata extraction.\n\n```python\nimport ffmpeg\n\ndef extract_audio_local(video_path: Path, output_dir: Path) -\u003e Path:\n    output = output_dir / 'audio.mp3'\n    \n    if output.exists():\n        return output  # Cache hit\n    \n    try:\n        (\n            ffmpeg\n            .input(str(video_path))\n            .output(str(output), acodec='libmp3lame', ab='128k', ac=1, ar=16000)\n            .overwrite_output()\n            .run(capture_stdout=True, capture_stderr=True)\n        )\n    except ffmpeg.Error as e:\n        raise RuntimeError(f'Audio extraction failed: {e.stderr.decode()}')\n    \n    return output\n```\n\n### Audio Settings for Whisper\n- 16kHz sample rate (whisper native)\n- Mono channel (speech doesn't need stereo)\n- 128kbps quality (plenty for speech)\n- MP3 format (universally compatible)\n\n### Handle Audio-Only Files\n```python\nprobe = ffmpeg.probe(video_path)\nhas_video = any(s['codec_type'] == 'video' for s in probe['streams'])\n# If no video, just convert audio format\n```","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:16:28.923093-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T09:45:59.83741-06:00","closed_at":"2026-02-01T09:45:59.83741-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-hqq","depends_on_id":"claudetube-2ag","type":"blocks","created_at":"2026-01-31T23:17:13.306627-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-hqq","depends_on_id":"claudetube-u4h","type":"parent-child","created_at":"2026-01-31T23:17:40.38723-06:00","created_by":"danielbarrett"}],"comments":[{"id":5,"issue_id":"claudetube-hqq","author":"danielbarrett","text":"Commit: ddba428d4b0649ca2bc2fc7de4d549931721f7c8","created_at":"2026-02-01T15:45:33Z"},{"id":6,"issue_id":"claudetube-hqq","author":"danielbarrett","text":"## What was done\n- Added `FFmpegTool.extract_audio()` method to `tools/ffmpeg.py`\n- Added `extract_audio_local()` function to `operations/download.py`\n- Exported new function from `operations/__init__.py`\n- Optimized for whisper: 16kHz mono, 128kbps MP3\n- Cache hit logic: skips if audio.mp3 exists\n- Files: `tools/ffmpeg.py`, `operations/download.py`, `operations/__init__.py`\n\n## Left undone\n- None\n\n## Gotchas\n- The `-vn` flag in ffmpeg gracefully handles both video+audio and audio-only inputs","created_at":"2026-02-01T15:45:52Z"}]}
{"id":"claudetube-i3x","title":"Implement VideoMemory class","description":"## User Story\nAs a user asking multiple questions about the same video, I want Claude to remember what it learned previously.\n\n## Acceptance Criteria\n- [ ] Caches observations Claude makes about scenes\n- [ ] Caches Q\u0026A pairs for future reference\n- [ ] Retrieves context when examining scenes\n- [ ] Persists across sessions\n- [ ] Stores in memory/observations.json and memory/qa_history.json\n\n## Technical Implementation\n\n### Library: Just JSON (stdlib)\nMemory is simple key-value storage - no external DB needed.\n\n```python\nimport json\nfrom pathlib import Path\nfrom datetime import datetime\nfrom dataclasses import dataclass, asdict\nfrom typing import Optional\n\n@dataclass\nclass Observation:\n    scene_id: int\n    type: str  # 'code_explanation', 'person_identified', 'error_found', etc.\n    content: str\n    timestamp: str = ''\n\n@dataclass\nclass QAPair:\n    question: str\n    answer: str\n    relevant_scenes: list[int]\n    timestamp: str = ''\n\nclass VideoMemory:\n    '''Persistent memory of what the agent has learned about a video.'''\n    \n    def __init__(self, video_id: str, cache_dir: Path):\n        self.video_id = video_id\n        self.memory_dir = cache_dir / 'memory'\n        self.memory_dir.mkdir(exist_ok=True)\n        \n        self.observations_file = self.memory_dir / 'observations.json'\n        self.qa_file = self.memory_dir / 'qa_history.json'\n        \n        self.observations = self._load_observations()\n        self.qa_history = self._load_qa()\n    \n    def _load_observations(self) -\u003e dict[int, list[dict]]:\n        if self.observations_file.exists():\n            return json.loads(self.observations_file.read_text())\n        return {}\n    \n    def _load_qa(self) -\u003e list[dict]:\n        if self.qa_file.exists():\n            return json.loads(self.qa_file.read_text())\n        return []\n    \n    def record_observation(self, scene_id: int, obs_type: str, content: str):\n        '''Record something the agent noticed.'''\n        if str(scene_id) not in self.observations:\n            self.observations[str(scene_id)] = []\n        \n        self.observations[str(scene_id)].append({\n            'type': obs_type,\n            'content': content,\n            'timestamp': datetime.now().isoformat()\n        })\n        self._save_observations()\n    \n    def record_qa(self, question: str, answer: str, scenes: list[int]):\n        '''Cache Q\u0026A for future reference.'''\n        self.qa_history.append({\n            'question': question,\n            'answer': answer,\n            'scenes': scenes,\n            'timestamp': datetime.now().isoformat()\n        })\n        self._save_qa()\n    \n    def get_context_for_scene(self, scene_id: int) -\u003e dict:\n        '''Get everything learned about a scene.'''\n        return {\n            'observations': self.observations.get(str(scene_id), []),\n            'related_qa': [\n                qa for qa in self.qa_history\n                if scene_id in qa['scenes']\n            ]\n        }\n    \n    def search_qa_history(self, query: str) -\u003e list[dict]:\n        '''Find relevant past Q\u0026A.'''\n        # Simple keyword matching\n        query_lower = query.lower()\n        return [\n            qa for qa in self.qa_history\n            if query_lower in qa['question'].lower() or query_lower in qa['answer'].lower()\n        ]\n    \n    def _save_observations(self):\n        self.observations_file.write_text(json.dumps(self.observations, indent=2))\n    \n    def _save_qa(self):\n        self.qa_file.write_text(json.dumps(self.qa_history, indent=2))\n```\n\n### Usage Pattern\n```python\n# When Claude examines a scene and learns something\nmemory = VideoMemory(video_id, cache_dir)\nmemory.record_observation(\n    scene_id=5,\n    obs_type='bug_identified',\n    content='Off-by-one error in the loop at line 42'\n)\n\n# When answering a question\nmemory.record_qa(\n    question='What bug was fixed?',\n    answer='An off-by-one error in the loop',\n    scenes=[5, 8, 12]\n)\n\n# Later, when revisiting scene 5\ncontext = memory.get_context_for_scene(5)\n# Returns previous observations and related Q\u0026A\n```","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:21:14.657199-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:31:23.978339-06:00","dependencies":[{"issue_id":"claudetube-i3x","depends_on_id":"claudetube-9hk","type":"parent-child","created_at":"2026-01-31T23:21:45.504721-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-i3x","depends_on_id":"claudetube-33e","type":"blocks","created_at":"2026-01-31T23:21:45.924211-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-j2j","title":"Multi-pass analysis depths","description":"## User Story\nAs a user, I want to control analysis depth - quick for simple questions, deep for complex investigation.\n\n## Acceptance Criteria\n- [ ] QUICK: scenes + transcript only (~2s)\n- [ ] STANDARD: + visual transcripts (~30s)\n- [ ] DEEP: + OCR, code extraction, entities (~2min)\n- [ ] EXHAUSTIVE: + frame-by-frame for focus sections\n- [ ] Commands: /yt:deep, /yt:focus\n\n## Technical Implementation\n\n### Analysis Depth Enum\n```python\nfrom enum import Enum\nfrom typing import Optional\n\nclass AnalysisDepth(Enum):\n    QUICK = 'quick'           # Scenes + transcript\n    STANDARD = 'standard'     # + visual transcripts\n    DEEP = 'deep'             # + OCR, code, entities\n    EXHAUSTIVE = 'exhaustive' # + frame-by-frame\n\ndef analyze_video(\n    video_id: str,\n    depth: AnalysisDepth = AnalysisDepth.STANDARD,\n    focus_sections: Optional[list[int]] = None\n) -\u003e dict:\n    '''Analyze video at specified depth.'''\n    \n    cache_dir = CACHE_BASE / video_id\n    scenes = load_scenes(cache_dir)\n    \n    # Quick: always done (scenes + transcript)\n    # Already have this from segmentation\n    \n    if depth == AnalysisDepth.QUICK:\n        return {'scenes': scenes, 'depth': 'quick'}\n    \n    # Standard: add visual transcripts\n    for scene in scenes:\n        if focus_sections and scene['segment_id'] not in focus_sections:\n            continue\n        if 'visual' not in scene:\n            scene['visual'] = generate_visual_transcript(scene)\n    \n    if depth == AnalysisDepth.STANDARD:\n        save_scenes(cache_dir, scenes)\n        return {'scenes': scenes, 'depth': 'standard'}\n    \n    # Deep: add technical content\n    for scene in scenes:\n        if focus_sections and scene['segment_id'] not in focus_sections:\n            continue\n        if 'technical' not in scene:\n            scene['technical'] = extract_technical_content(scene)\n        if 'entities' not in scene:\n            scene['entities'] = extract_entities(scene)\n    \n    if depth == AnalysisDepth.DEEP:\n        save_scenes(cache_dir, scenes)\n        return {'scenes': scenes, 'depth': 'deep'}\n    \n    # Exhaustive: frame-by-frame for focus sections\n    if focus_sections:\n        for scene in scenes:\n            if scene['segment_id'] in focus_sections:\n                scene['frame_analysis'] = analyze_all_frames(scene)\n    \n    save_scenes(cache_dir, scenes)\n    return {'scenes': scenes, 'depth': 'exhaustive'}\n```\n\n### MCP Tools\n```python\n@mcp.tool()\ndef analyze_deep_tool(video_id: str, question: str) -\u003e str:\n    '''Deep analysis of video with OCR and entity extraction.'''\n    result = analyze_video(video_id, AnalysisDepth.DEEP)\n    # Then answer question using enriched scenes\n    return answer_question(result['scenes'], question)\n\n@mcp.tool()\ndef analyze_focus_tool(\n    video_id: str,\n    start_time: float,\n    end_time: float,\n    question: str\n) -\u003e str:\n    '''Exhaustive analysis of a specific video section.'''\n    # Find scenes in time range\n    scenes = load_scenes(CACHE_BASE / video_id)\n    focus_ids = [\n        s['segment_id'] for s in scenes\n        if s['start'] \u003e= start_time and s['end'] \u003c= end_time\n    ]\n    \n    result = analyze_video(video_id, AnalysisDepth.EXHAUSTIVE, focus_ids)\n    return answer_question(result['scenes'], question)\n```\n\n### Cost/Time Tradeoffs\n| Depth | Time (30min video) | API Calls | Cost |\n|-------|-------------------|-----------|------|\n| QUICK | ~2s | 0 | $0 |\n| STANDARD | ~30s | ~15 vision | ~$0.15 |\n| DEEP | ~2min | ~30 vision + OCR | ~$0.50 |\n| EXHAUSTIVE | ~5min+ | ~100+ | ~$2+ |","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:21:19.444481-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:31:39.274801-06:00","dependencies":[{"issue_id":"claudetube-j2j","depends_on_id":"claudetube-9hk","type":"parent-child","created_at":"2026-01-31T23:21:45.609437-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-j2j","depends_on_id":"claudetube-vs1","type":"blocks","created_at":"2026-01-31T23:21:46.032036-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-ji6","title":"Add /yt:scenes command","description":"## User Story\nAs a user, I want to see the scene structure of a video so I can understand its organization before asking questions.\n\n## Acceptance Criteria\n- [ ] Returns list of scenes with ID, start/end, duration\n- [ ] Includes transcript summary per scene\n- [ ] Includes visual description if available\n- [ ] Includes detected elements (people, objects, text)\n- [ ] Formatted for Claude consumption\n\n## Technical Implementation\n\n### MCP Tool Definition\n```python\n@mcp.tool()\ndef get_scenes_tool(video_id: str) -\u003e str:\n    '''Get scene structure of a processed video.\n    \n    Returns scene list with timestamps, transcript summaries,\n    and visual descriptions for understanding video structure.\n    '''\n    cache_dir = CACHE_BASE / video_id\n    scenes_file = cache_dir / 'scenes' / 'scenes.json'\n    \n    if not scenes_file.exists():\n        return json.dumps({\n            'error': 'Video not segmented. Run process_video first.',\n            'video_id': video_id\n        })\n    \n    scenes = json.loads(scenes_file.read_text())\n    \n    # Enrich with visual transcripts if available\n    for scene in scenes['segments']:\n        scene_dir = cache_dir / 'scenes' / f\"scene_{scene['segment_id']:03d}\"\n        visual_file = scene_dir / 'visual.json'\n        if visual_file.exists():\n            scene['visual'] = json.loads(visual_file.read_text())\n    \n    return json.dumps(scenes, indent=2)\n```\n\n### Output Format\n```json\n{\n  \"video_id\": \"abc123\",\n  \"method\": \"transcript\",\n  \"boundary_count\": 8,\n  \"segments\": [\n    {\n      \"segment_id\": 0,\n      \"start\": 0,\n      \"end\": 45.2,\n      \"duration\": 45.2,\n      \"transcript_text\": \"Welcome to this tutorial on React hooks...\",\n      \"visual\": {\n        \"description\": \"Person at desk with laptop showing VS Code\",\n        \"people\": [\"presenter\"],\n        \"text_on_screen\": [\"React Hooks Tutorial\"]\n      }\n    },\n    ...\n  ]\n}\n```\n\n### Skill Command\nAdd to claudetube skills:\n```\n/yt:scenes \u003cvideo_id\u003e\n```","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:19:19.097911-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:49:42.783476-06:00","closed_at":"2026-02-01T11:49:42.783476-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-ji6","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:36.233698-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-ji6","depends_on_id":"claudetube-vs1","type":"blocks","created_at":"2026-01-31T23:19:44.860128-06:00","created_by":"danielbarrett"}],"comments":[{"id":45,"issue_id":"claudetube-ji6","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\n/yt:scenes should return INSTANTLY for processed videos:\n1. **CACHE** - scenes/scenes.json exists? Return it.\n2. **CHAPTERS** - Extract from yt-dlp metadata (instant).\n3. **PROCESS** - Run smart segmentation only if cache miss.\n\nTarget: \u003c100ms for cached videos. User should never wait for scene detection.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:12:48Z"},{"id":57,"issue_id":"claudetube-ji6","author":"danielbarrett","text":"## What was done\n- Added `get_scenes` MCP tool in `src/claudetube/mcp_server.py`\n- Added `/yt:scenes` skill command in `commands/yt/scenes.md`\n- Implements 'Cheap First, Expensive Last': returns cached scenes instantly, runs segmentation only if needed\n- Files: src/claudetube/mcp_server.py, commands/yt/scenes.md\n\n## Left undone\n- Visual enrichment (visual.json) requires separate visual processing - not triggered by this command\n\n## Gotchas\n- state.json doesn't store native YouTube chapters (only description), so chapter extraction works via description parsing\n- The segmentation uses transcript segments from SRT + description for chapter hints\n\nCommit: b6c799b289425415597006c555153cf9c23ad212","created_at":"2026-02-01T17:49:34Z"}]}
{"id":"claudetube-jmu","title":"Extract YouTube chapters from video metadata","description":"## User Story\nAs a user analyzing YouTube videos, I want chapter markers used automatically for segmentation (they're human-curated and free).\n\n## Acceptance Criteria\n- [ ] Extracts chapters from yt-dlp metadata (video_info['chapters'])\n- [ ] Parses chapters from description (format: '0:00 Introduction')\n- [ ] Returns structured list with title, start, end, confidence\n- [ ] Sets source='youtube_chapters' (highest confidence=0.95)\n- [ ] Graceful handling when no chapters exist\n\n## Technical Implementation\n\n### Library: Already using yt-dlp\nChapters come free with yt-dlp metadata - no additional dependencies.\n\n```python\nimport re\n\ndef extract_youtube_chapters(video_info: dict) -\u003e list[dict]:\n    chapters = []\n    \n    # Method 1: Native chapters from yt-dlp\n    if video_info.get('chapters'):\n        for ch in video_info['chapters']:\n            chapters.append({\n                'title': ch['title'],\n                'start': ch['start_time'],\n                'end': ch.get('end_time'),\n                'source': 'youtube_chapters',\n                'confidence': 0.95\n            })\n        return chapters\n    \n    # Method 2: Parse from description\n    description = video_info.get('description', '')\n    pattern = r'(\\d{1,2}:\\d{2}(?::\\d{2})?)\\s*[-–—]?\\s*(.+?)(?:\\n|$)'\n    \n    for ts, title in re.findall(pattern, description):\n        chapters.append({\n            'title': title.strip(),\n            'start': parse_timestamp(ts),\n            'source': 'description_parsed',\n            'confidence': 0.9\n        })\n    \n    return chapters\n\ndef parse_timestamp(ts: str) -\u003e float:\n    '''Convert '1:23:45' or '1:23' to seconds.'''\n    parts = list(map(int, ts.split(':')))\n    if len(parts) == 3:\n        return parts[0]*3600 + parts[1]*60 + parts[2]\n    return parts[0]*60 + parts[1]\n```\n\n### Why Highest Confidence\n- Human-curated = intentional boundaries\n- Creator knows content best\n- Already timestamp-aligned","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:18:21.304871-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T09:49:55.687005-06:00","closed_at":"2026-02-01T09:49:55.687005-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-jmu","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:34.819279-06:00","created_by":"danielbarrett"}],"comments":[{"id":7,"issue_id":"claudetube-jmu","author":"danielbarrett","text":"Commit: 7a52f842768bbc7091410a7e84ac1414d0b24703","created_at":"2026-02-01T15:49:30Z"},{"id":8,"issue_id":"claudetube-jmu","author":"danielbarrett","text":"## What was done\n- Created Chapter dataclass model with title, start, end, source, confidence\n- Implemented extract_youtube_chapters() function:\n  - Method 1: Native yt-dlp chapters (confidence 0.95)\n  - Method 2: Description timestamp parsing (confidence 0.9)\n- Implemented parse_timestamp() helper for HH:MM:SS formats\n- Added comprehensive tests (23 tests passing)\n- Files: models/chapter.py, operations/chapters.py, tests/test_chapters.py\n\n## Left undone\n- None\n\n## Gotchas\n- Empty chapters list in metadata falls through to description parsing\n- End times auto-filled from next chapter start or video duration","created_at":"2026-02-01T15:49:48Z"}]}
{"id":"claudetube-jns","title":"Detect narrative structure","description":"## User Story\nAs a user, I want to understand the high-level structure of a video (intro, main sections, conclusion).\n\n## Acceptance Criteria\n- [ ] Clusters scenes by topic similarity\n- [ ] Labels sections: introduction, main content, conclusion\n- [ ] Detects video type: tutorial, lecture, demo, interview\n- [ ] Stores in structure/narrative.json\n\n## Technical Implementation\n\n### Library: scikit-learn (already a dependency)\n```python\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nimport numpy as np\n\ndef detect_narrative_structure(scenes: list[dict]) -\u003e dict:\n    '''Identify high-level structure of video.'''\n    \n    # Get embeddings\n    embeddings = np.array([s['embedding'] for s in scenes if 'embedding' in s])\n    \n    if len(embeddings) \u003c 3:\n        return {'sections': [{'summary': 'Single section', 'scenes': list(range(len(scenes)))}]}\n    \n    # Find optimal number of clusters\n    best_k = find_optimal_clusters(embeddings, max_k=min(8, len(embeddings) // 2))\n    \n    # Cluster scenes\n    clustering = AgglomerativeClustering(n_clusters=best_k, linkage='ward')\n    labels = clustering.fit_predict(embeddings)\n    \n    # Build sections (preserve temporal order)\n    sections = build_sections(scenes, labels)\n    \n    # Classify video type\n    video_type = classify_video_type(scenes, sections)\n    \n    return {\n        'type': video_type,\n        'sections': sections,\n        'cluster_count': best_k\n    }\n\ndef find_optimal_clusters(embeddings: np.ndarray, max_k: int) -\u003e int:\n    '''Find optimal number of clusters using silhouette score.'''\n    best_score = -1\n    best_k = 2\n    \n    for k in range(2, max_k + 1):\n        clustering = AgglomerativeClustering(n_clusters=k)\n        labels = clustering.fit_predict(embeddings)\n        score = silhouette_score(embeddings, labels)\n        \n        if score \u003e best_score:\n            best_score = score\n            best_k = k\n    \n    return best_k\n```\n\n### Section Building\n```python\ndef build_sections(scenes: list[dict], labels: list[int]) -\u003e list[dict]:\n    '''Group scenes into sections preserving temporal order.'''\n    \n    sections = []\n    current_label = labels[0]\n    current_scenes = [scenes[0]]\n    \n    for scene, label in zip(scenes[1:], labels[1:]):\n        if label != current_label:\n            # New section\n            sections.append(create_section(current_scenes, len(sections)))\n            current_scenes = [scene]\n            current_label = label\n        else:\n            current_scenes.append(scene)\n    \n    # Last section\n    sections.append(create_section(current_scenes, len(sections)))\n    \n    return sections\n\ndef create_section(scenes: list[dict], idx: int) -\u003e dict:\n    '''Create section summary from scenes.'''\n    return {\n        'section_id': idx,\n        'start': scenes[0]['start'],\n        'end': scenes[-1]['end'],\n        'scene_ids': [s['segment_id'] for s in scenes],\n        'summary': summarize_scenes(scenes),\n        'label': infer_section_label(scenes, idx, total_sections)\n    }\n```\n\n### Video Type Classification\n```python\ndef classify_video_type(scenes: list[dict], sections: list[dict]) -\u003e str:\n    '''Classify video type from content patterns.'''\n    \n    # Count content types\n    content_types = [s.get('technical', {}).get('content_type', 'unknown') for s in scenes]\n    type_counts = {t: content_types.count(t) for t in set(content_types)}\n    \n    # Heuristics\n    if type_counts.get('code', 0) \u003e len(scenes) * 0.3:\n        return 'coding_tutorial'\n    elif type_counts.get('slides', 0) \u003e len(scenes) * 0.5:\n        return 'lecture'\n    elif type_counts.get('talking_head', 0) \u003e len(scenes) * 0.7:\n        return 'interview'\n    elif len(sections) \u003e 5:\n        return 'tutorial'\n    else:\n        return 'demo'\n```","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:20:48.648575-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:30:50.229081-06:00","dependencies":[{"issue_id":"claudetube-jns","depends_on_id":"claudetube-sa5","type":"parent-child","created_at":"2026-01-31T23:21:04.709902-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-jns","depends_on_id":"claudetube-uzo","type":"blocks","created_at":"2026-01-31T23:21:05.314498-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-js6","title":"Check for embedded subtitles in local files","description":"## User Story\nAs a user with videos containing embedded or sidecar subtitles, I want those used automatically instead of running whisper (faster, often higher quality).\n\n## Acceptance Criteria\n- [ ] Detects embedded subtitle streams via ffprobe\n- [ ] Extracts embedded subs to audio.srt\n- [ ] Checks for sidecar files: video.srt, video.vtt, video.ass\n- [ ] Converts ASS/VTT to SRT format\n- [ ] Sets transcript_source='embedded' or 'sidecar'\n- [ ] Falls back to whisper if no subs found\n\n## Technical Implementation\n\n### Library: ffmpeg-python + pysubs2\n```bash\npip install pysubs2  # 100k+ downloads/month - subtitle format conversion\n```\n\n### Detect Embedded Subtitles\n```python\nimport ffmpeg\n\ndef find_embedded_subtitles(video_path: str) -\u003e list:\n    probe = ffmpeg.probe(video_path)\n    return [\n        s for s in probe['streams'] \n        if s['codec_type'] == 'subtitle'\n    ]\n```\n\n### Extract Embedded Subtitles\n```python\ndef extract_embedded_subs(video_path: Path, output: Path) -\u003e bool:\n    subs = find_embedded_subtitles(str(video_path))\n    if not subs:\n        return False\n    \n    # Extract first subtitle track\n    ffmpeg.input(str(video_path)).output(\n        str(output), map=f'0:s:0'\n    ).run()\n    return True\n```\n\n### Check Sidecar Files\n```python\ndef find_sidecar_subs(video_path: Path) -\u003e Path | None:\n    for ext in ['.srt', '.vtt', '.ass', '.ssa']:\n        sidecar = video_path.with_suffix(ext)\n        if sidecar.exists():\n            return sidecar\n    return None\n```\n\n### Convert to SRT\n```python\nimport pysubs2\n\ndef convert_to_srt(input_path: Path, output_path: Path):\n    subs = pysubs2.load(str(input_path))\n    subs.save(str(output_path))  # Auto-detects format from extension\n```","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:16:32.977486-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T10:56:24.63161-06:00","closed_at":"2026-02-01T10:56:24.63161-06:00","close_reason":"Implemented embedded/sidecar subtitle detection","dependencies":[{"issue_id":"claudetube-js6","depends_on_id":"claudetube-2ag","type":"blocks","created_at":"2026-01-31T23:17:13.433719-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-js6","depends_on_id":"claudetube-u4h","type":"parent-child","created_at":"2026-01-31T23:17:40.607266-06:00","created_by":"danielbarrett"}],"comments":[{"id":21,"issue_id":"claudetube-js6","author":"danielbarrett","text":"Commit: 7b798e72fbd9fdcef90c1e93d66b37485149b48a\n\n## What was done\n- Added operations/subtitles.py (217 lines) for subtitle detection/extraction\n- Detects embedded subtitle streams via ffprobe\n- Finds sidecar files (.srt, .vtt, .ass, .ssa)\n- Extracts and converts to SRT format via pysubs2\n- Integrated into process_local_video() pipeline\n- Falls back to whisper if no subtitles found\n- Added comprehensive tests\n\nFiles: operations/subtitles.py, operations/processor.py, pyproject.toml, tests/test_subtitles_local.py\n\n## Left undone\n- None\n\n## Gotchas\n- pysubs2 handles format conversion cleanly\n- Embedded subs require ffmpeg extraction before conversion","created_at":"2026-02-01T16:56:16Z"}]}
{"id":"claudetube-jzb","title":"Cross-video knowledge graph for playlists","description":"## User Story\nAs a user watching course playlists, I want semantic links between videos so the system understands 'In the previous video...' context.\n\n## Acceptance Criteria\n- [ ] Extracts shared entities across video titles/descriptions\n- [ ] Builds prerequisite chains for courses (video N requires videos 1..N-1)\n- [ ] Links videos by shared topics\n- [ ] Creates symlinks to video caches\n- [ ] Stores in playlists/{PLAYLIST_ID}/knowledge_graph.json\n\n## Technical Implementation\n\n### Entity Extraction: spaCy (Industry standard NER)\n```bash\npip install spacy\npython -m spacy download en_core_web_sm  # Small model, fast\n```\n\n```python\nimport spacy\n\nnlp = spacy.load('en_core_web_sm')\n\ndef extract_named_entities(text: str) -\u003e list[dict]:\n    '''Extract named entities from text.'''\n    doc = nlp(text)\n    return [\n        {'text': ent.text, 'label': ent.label_}\n        for ent in doc.ents\n    ]\n```\n\n### Alternative: Just keyword extraction (lighter)\n```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef extract_topic_keywords(texts: list[str], top_n: int = 10) -\u003e list[str]:\n    '''Extract important keywords via TF-IDF.'''\n    vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n    tfidf = vectorizer.fit_transform(texts)\n    \n    # Sum TF-IDF scores across documents\n    scores = tfidf.sum(axis=0).A1\n    top_indices = scores.argsort()[-top_n:][::-1]\n    \n    return [vectorizer.get_feature_names_out()[i] for i in top_indices]\n```\n\n### Build Knowledge Graph\n```python\nfrom pathlib import Path\n\ndef build_playlist_knowledge_graph(playlist_context: dict, cache_base: Path) -\u003e dict:\n    videos = playlist_context['videos']\n    \n    # Extract shared entities\n    all_text = ' '.join(v['title'] + ' ' + v.get('description', '') for v in videos)\n    common_entities = extract_named_entities(all_text)\n    \n    # Build prerequisite chain for courses\n    if playlist_context['inferred_type'] == 'course':\n        for i, video in enumerate(videos):\n            video['prerequisites'] = [videos[j]['video_id'] for j in range(i)]\n            video['next'] = videos[i+1]['video_id'] if i \u003c len(videos)-1 else None\n    \n    # Create symlinks to video caches\n    playlist_dir = cache_base / 'playlists' / playlist_context['playlist_id'] / 'videos'\n    playlist_dir.mkdir(parents=True, exist_ok=True)\n    \n    for video in videos:\n        video_cache = cache_base / video['video_id']\n        if video_cache.exists():\n            symlink = playlist_dir / video['video_id']\n            if not symlink.exists():\n                symlink.symlink_to(video_cache)\n    \n    return {\n        'playlist': playlist_context,\n        'common_entities': common_entities,\n        'videos': videos\n    }\n```\n\n### Recommendation\n- Use TF-IDF keyword extraction (lighter, no spacy download)\n- Add spacy as optional for richer entity extraction\n- Prerequisite chain is most valuable for courses","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-31T23:19:14.916667-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:28:17.353824-06:00","dependencies":[{"issue_id":"claudetube-jzb","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:36.122939-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-jzb","depends_on_id":"claudetube-asm","type":"blocks","created_at":"2026-01-31T23:19:44.96892-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-krh","title":"OCR extraction for technical content","description":"## User Story\nAs a user analyzing technical videos, I need text extracted from screen (code, slides, terminal output) for searchability.\n\n## Acceptance Criteria\n- [ ] Extracts all visible text via OCR\n- [ ] Returns text with bounding boxes\n- [ ] Handles code (monospace fonts, syntax highlighting)\n- [ ] Handles slides/presentations\n- [ ] Handles terminal output\n- [ ] Stores in scenes/scene_XXX/technical.json\n\n## Technical Implementation\n\n### Library: EasyOCR (Better for code than Tesseract)\n```bash\npip install easyocr  # 5M+ downloads/month\n```\n\n```python\nimport easyocr\n\n# Initialize once (downloads models on first run)\nreader = easyocr.Reader(['en'], gpu=False)  # CPU mode for compatibility\n\ndef extract_text_from_frame(frame_path: str) -\u003e list[dict]:\n    '''Extract all text with positions.'''\n    results = reader.readtext(frame_path)\n    \n    return [\n        {\n            'text': text,\n            'confidence': conf,\n            'bbox': {\n                'x1': int(bbox[0][0]), 'y1': int(bbox[0][1]),\n                'x2': int(bbox[2][0]), 'y2': int(bbox[2][1])\n            }\n        }\n        for bbox, text, conf in results\n        if conf \u003e 0.5  # Filter low confidence\n    ]\n```\n\n### Alternative: PaddleOCR (Faster, also excellent)\n```bash\npip install paddlepaddle paddleocr\n```\n```python\nfrom paddleocr import PaddleOCR\nocr = PaddleOCR(use_angle_cls=True, lang='en')\n```\n\n### Alternative: Tesseract (Classic, widely available)\n```bash\npip install pytesseract\nbrew install tesseract  # macOS\n```\n\n### Recommendation: EasyOCR\n- Best accuracy for code/technical text\n- Handles multiple fonts well\n- Pure Python (no system deps except torch)\n- GPU optional\n\n### Content Type Classification\n```python\ndef classify_frame_content(ocr_results: list, frame: np.ndarray) -\u003e str:\n    '''Classify frame as: code, slides, terminal, diagram, talking_head.'''\n    \n    text_coverage = sum(r['bbox_area'] for r in ocr_results) / frame_area(frame)\n    \n    # Heuristics\n    if text_coverage \u003e 0.5 and has_monospace_font(frame):\n        return 'code'\n    elif text_coverage \u003e 0.3 and has_large_text(ocr_results):\n        return 'slides'\n    elif has_dark_background(frame) and text_coverage \u003e 0.2:\n        return 'terminal'\n    elif text_coverage \u003c 0.1:\n        return 'talking_head'\n    else:\n        return 'diagram'\n```","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:19:02.061994-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:53:29.111782-06:00","closed_at":"2026-02-01T11:53:29.111782-06:00","close_reason":"Implemented OCR extraction with EasyOCR. Added text likelihood estimation (cheap first), content type classification, and cache support. 9 tests passing.","dependencies":[{"issue_id":"claudetube-krh","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:35.802149-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-krh","depends_on_id":"claudetube-33e","type":"blocks","created_at":"2026-01-31T23:19:44.598435-06:00","created_by":"danielbarrett"}],"comments":[{"id":39,"issue_id":"claudetube-krh","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nOCR is moderately expensive. Follow this hierarchy:\n1. **CACHE** - OCR results cached for this frame/scene? Use them.\n2. **DETECT** - Only run OCR on frames that likely contain text (code, slides).\n3. **SKIP** - Don't OCR talking-head frames or outdoor scenes.\n\nUse cheap heuristics (frame variance, aspect ratio) to detect text-heavy frames before running OCR.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:12:17Z"},{"id":55,"issue_id":"claudetube-krh","author":"danielbarrett","text":"Note from @danielbarrett: Claude's vision API likely does better OCR than standalone libs like EasyOCR. Consider using Claude API directly for OCR extraction as the primary approach.","created_at":"2026-02-01T17:44:57Z"}]}
{"id":"claudetube-kvk","title":"Create multimodal scene embeddings","description":"## User Story\nAs a user searching for specific moments, I need scenes embedded as vectors so semantic search works.\n\n## Acceptance Criteria\n- [ ] Creates unified embedding per scene (visual + audio + text)\n- [ ] Supports Voyage AI multimodal-3 (best quality)\n- [ ] Supports local fallback (CLIP + sentence-transformers)\n- [ ] Stores in embeddings/scene_embeddings.npy\n- [ ] Config via CLAUDETUBE_EMBEDDING_MODEL env var\n\n## Technical Implementation\n\n### Option A: Voyage AI (Recommended - Best Quality)\n```bash\npip install voyageai  # Official Voyage client\n```\n\n```python\nimport voyageai\nfrom PIL import Image\nimport numpy as np\n\nvoyage = voyageai.Client()  # Uses VOYAGE_API_KEY env var\n\ndef embed_scene_voyage(scene: dict, keyframe_paths: list[str]) -\u003e np.ndarray:\n    '''Create multimodal embedding with Voyage.'''\n    \n    # Combine text content\n    text_content = f'''\n    Scene {scene['segment_id']} ({scene['start']:.1f}s - {scene['end']:.1f}s)\n    \n    AUDIO: {scene.get('transcript_text', '')}\n    \n    VISUAL: {scene.get('visual', {}).get('description', '')}\n    \n    TEXT ON SCREEN: {' '.join(scene.get('technical', {}).get('ocr_text', []))}\n    '''\n    \n    # Load keyframe images\n    images = [Image.open(p) for p in keyframe_paths[:3]]  # Max 3 images\n    \n    # Multimodal embedding\n    result = voyage.multimodal_embed(\n        inputs=[[text_content] + images],\n        model='voyage-multimodal-3',\n        input_type='document'\n    )\n    \n    return np.array(result.embeddings[0])\n```\n\n### Option B: Local Fallback (CLIP + Sentence-Transformers)\n```bash\npip install sentence-transformers open-clip-torch\n```\n\n```python\nfrom sentence_transformers import SentenceTransformer\nimport open_clip\nimport torch\n\n# Load models once\ntext_model = SentenceTransformer('all-MiniLM-L6-v2')\nclip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n\ndef embed_scene_local(scene: dict, keyframe_paths: list[str]) -\u003e np.ndarray:\n    '''Create embedding using local models.'''\n    \n    # Text embedding\n    text = scene.get('transcript_text', '') + ' ' + scene.get('visual', {}).get('description', '')\n    text_emb = text_model.encode(text)\n    \n    # Image embeddings (average)\n    img_embs = []\n    for path in keyframe_paths[:3]:\n        img = preprocess(Image.open(path)).unsqueeze(0)\n        with torch.no_grad():\n            img_emb = clip_model.encode_image(img).squeeze().numpy()\n        img_embs.append(img_emb)\n    \n    avg_img_emb = np.mean(img_embs, axis=0) if img_embs else np.zeros(512)\n    \n    # Concatenate (text: 384d, image: 512d = 896d total)\n    return np.concatenate([text_emb, avg_img_emb])\n```\n\n### Config Pattern\n```python\nimport os\n\ndef embed_scene(scene: dict, keyframe_paths: list[str]) -\u003e np.ndarray:\n    model = os.environ.get('CLAUDETUBE_EMBEDDING_MODEL', 'voyage')\n    \n    if model == 'voyage':\n        return embed_scene_voyage(scene, keyframe_paths)\n    else:\n        return embed_scene_local(scene, keyframe_paths)\n```\n\n### Storage\n```python\ndef save_embeddings(cache_dir: Path, embeddings: list[np.ndarray], scene_ids: list[int]):\n    emb_dir = cache_dir / 'embeddings'\n    emb_dir.mkdir(exist_ok=True)\n    \n    np.save(emb_dir / 'scene_embeddings.npy', np.array(embeddings))\n    (emb_dir / 'scene_ids.json').write_text(json.dumps(scene_ids))\n```","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:19:55.365743-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T12:06:55.375742-06:00","closed_at":"2026-02-01T12:06:55.375742-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-kvk","depends_on_id":"claudetube-dth","type":"parent-child","created_at":"2026-01-31T23:20:24.533614-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-kvk","depends_on_id":"claudetube-33e","type":"blocks","created_at":"2026-01-31T23:20:25.108729-06:00","created_by":"danielbarrett"}],"comments":[{"id":37,"issue_id":"claudetube-kvk","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nEmbeddings are EXPENSIVE. Follow this hierarchy:\n1. **CACHE** - Embeddings already computed for this scene? Use them.\n2. **TEXT-ONLY** - Transcript embeddings are cheaper than multimodal.\n3. **MULTIMODAL** - Add visual embeddings only when text search fails.\n\nCompute incrementally: only embed new/changed scenes, not entire videos.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:12:07Z"},{"id":61,"issue_id":"claudetube-kvk","author":"danielbarrett","text":"Commit: 810e3a0cb420dd4d8e138566cc02c37b23a1e79b","created_at":"2026-02-01T18:06:36Z"},{"id":62,"issue_id":"claudetube-kvk","author":"danielbarrett","text":"## What was done\n- Created `src/claudetube/analysis/embeddings.py` with multimodal embedding support\n- Implemented Voyage AI integration for voyage-multimodal-3 (1024-dim)\n- Implemented local fallback using CLIP ViT-B-32 + sentence-transformers (896-dim)\n- Added storage functions (save_embeddings, load_embeddings, has_embeddings)\n- Config via CLAUDETUBE_EMBEDDING_MODEL env var (\"voyage\" or \"local\")\n- Added comprehensive tests in `tests/test_embeddings.py`\n- Updated `analysis/__init__.py` to export new functions\n\nFiles: src/claudetube/analysis/embeddings.py, src/claudetube/analysis/__init__.py, tests/test_embeddings.py\n\n## Left undone\n- None - all acceptance criteria met\n\n## Gotchas\n- Voyage AI requires VOYAGE_API_KEY env var and voyageai package\n- Local embedding requires sentence-transformers (required) and open-clip-torch (optional for images)\n- Local model falls back to text-only embedding (with zero image vector) if CLIP not available\n- Storage uses embeddings/scene_embeddings.npy with scene_ids.json for mapping","created_at":"2026-02-01T18:06:48Z"}]}
{"id":"claudetube-lk8","title":"Add process_local_file MCP tool","description":"## User Story\nAs a Claude Code user, I want to process local video files through the MCP interface with the same workflow as URL-based videos.\n\n## Acceptance Criteria\n- [ ] MCP tool accepts local file paths\n- [ ] Returns same VideoResult structure as process_video_tool\n- [ ] Works with process_video_tool (unified interface) OR separate tool\n- [ ] Validates file exists before processing\n- [ ] Clear error messages for invalid files\n\n## Technical Implementation\n\n### Approach: Unified Interface (Recommended)\nModify existing process_video_tool to handle both URLs and local files:\n\n```python\n@mcp.tool()\ndef process_video_tool(\n    url_or_path: str,\n    whisper_model: str = 'tiny',\n    copy: bool = False  # New param for local files\n) -\u003e str:\n    '''Process a video from URL or local file path.'''\n    \n    if is_local_file(url_or_path):\n        return process_local_video(url_or_path, whisper_model, copy)\n    else:\n        return process_remote_video(url_or_path, whisper_model)\n```\n\n### Alternative: Separate Tool\n```python\n@mcp.tool()\ndef process_local_video_tool(\n    path: str,\n    whisper_model: str = 'tiny',\n    copy: bool = False\n) -\u003e str:\n    '''Process a local video file.'''\n    ...\n```\n\n### Recommended: Unified Interface\n- Simpler mental model for users\n- Same return format regardless of source\n- 'Just works' with file paths or URLs\n\n### Pipeline Integration\n```python\ndef process_local_video(path: str, whisper_model: str, copy: bool) -\u003e VideoResult:\n    video_id = generate_local_video_id(path)\n    output_dir = CACHE_BASE / video_id\n    output_dir.mkdir(exist_ok=True)\n    \n    # 1. Cache the file (symlink or copy)\n    cached_file = cache_local_file(Path(path), output_dir, copy)\n    \n    # 2. Extract metadata via ffprobe\n    metadata = get_local_metadata(path)\n    \n    # 3. Check for embedded/sidecar subtitles\n    transcript = find_existing_subtitles(path, output_dir)\n    \n    # 4. If no subs, extract audio and transcribe\n    if not transcript:\n        audio = extract_audio_local(cached_file, output_dir)\n        transcript = transcribe_whisper(audio, whisper_model)\n    \n    # 5. Generate thumbnail\n    has_thumb = generate_thumbnail_local(cached_file, output_dir, metadata['duration'])\n    \n    # 6. Save state and return\n    save_state(output_dir, metadata, transcript, has_thumb)\n    return VideoResult(...)\n```","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:16:44.856174-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T10:41:31.064815-06:00","closed_at":"2026-02-01T10:41:31.064815-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-lk8","depends_on_id":"claudetube-6g0","type":"blocks","created_at":"2026-01-31T23:17:13.865938-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-lk8","depends_on_id":"claudetube-hqq","type":"blocks","created_at":"2026-01-31T23:17:14.032172-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-lk8","depends_on_id":"claudetube-u4h","type":"parent-child","created_at":"2026-01-31T23:17:41.008338-06:00","created_by":"danielbarrett"}],"comments":[{"id":15,"issue_id":"claudetube-lk8","author":"danielbarrett","text":"Commit: f2dba5bc1ff23cc20b63845c1d8ad45b8d749a41","created_at":"2026-02-01T16:41:09Z"},{"id":16,"issue_id":"claudetube-lk8","author":"danielbarrett","text":"## What was done\n- Added `process_local_video()` function in `operations/processor.py`\n  - Parses and validates local file paths via LocalFile model\n  - Caches files via symlink (default) or copy\n  - Extracts metadata via ffprobe (duration, dimensions, fps, codec)\n  - Generates thumbnail at 10% of video duration\n  - Extracts audio and transcribes with whisper\n  - Returns same VideoResult structure as URL-based processing\n  \n- Updated `process_video_tool` MCP endpoint to handle both URLs and local files\n  - Uses `is_local_file()` to detect local paths\n  - Routes to appropriate processor function\n  - Added `copy` parameter for local file caching strategy\n  - Updated docstring to document local file support\n\nFiles:\n- src/claudetube/operations/processor.py (added 171 lines)\n- src/claudetube/mcp_server.py (updated)\n- src/claudetube/operations/__init__.py (exports)\n\n## Left undone\n- None (all acceptance criteria met)\n\n## Gotchas\n- Local files reuse existing infrastructure: LocalFile model, FFprobeTool, extract_audio_local\n- Thumbnail generation uses FFmpegTool.extract_frame() at 10% of duration (or 5s max)\n- Cache validation checks symlink validity before returning cache hits","created_at":"2026-02-01T16:41:24Z"}]}
{"id":"claudetube-mnq","title":"Code block detection and language identification","description":"## User Story\nAs a user watching coding tutorials, I need extracted code classified by language with the actual code content preserved.\n\n## Acceptance Criteria\n- [ ] Detects code regions in frames\n- [ ] Identifies programming language (Python, JS, etc.)\n- [ ] Extracts code content as text\n- [ ] Returns structured code blocks with language + content + bbox\n\n## Technical Implementation\n\n### Language Detection: pygments (stdlib-ish, very reliable)\n```bash\npip install pygments  # 50M+ downloads/month - THE syntax highlighter\n```\n\n```python\nfrom pygments.lexers import guess_lexer, get_lexer_by_name\nfrom pygments.util import ClassNotFound\n\ndef detect_code_language(text: str) -\u003e str | None:\n    '''Detect programming language from code snippet.'''\n    try:\n        lexer = guess_lexer(text)\n        return lexer.name.lower()\n    except ClassNotFound:\n        return None\n```\n\n### Code Region Detection\nHeuristics for identifying code vs prose:\n\n```python\nimport re\n\nCODE_PATTERNS = [\n    r'def\\s+\\w+\\s*\\(',           # Python function\n    r'function\\s+\\w+\\s*\\(',      # JS function\n    r'class\\s+\\w+',                # Class definition\n    r'import\\s+\\w+',               # Import statement\n    r'=\u003e',                           # Arrow function\n    r'\\{\\s*\\}',                    # Empty braces\n    r'\\[\\s*\\]',                    # Empty brackets\n    r'\\w+\\s*=\\s*[\"\\']',          # Variable assignment\n    r'//|#|/\\*',                    # Comments\n    r'\\bif\\s*\\(|\\bfor\\s*\\(',    # Control structures\n]\n\ndef is_likely_code(text: str) -\u003e bool:\n    '''Check if text looks like code.'''\n    code_signals = sum(1 for p in CODE_PATTERNS if re.search(p, text))\n    return code_signals \u003e= 2 or (code_signals \u003e= 1 and len(text) \u003e 50)\n```\n\n### Full Pipeline\n```python\ndef extract_code_blocks(ocr_results: list[dict]) -\u003e list[dict]:\n    '''Group OCR results into code blocks.'''\n    \n    code_blocks = []\n    \n    # Group nearby text into blocks\n    blocks = cluster_text_by_position(ocr_results)\n    \n    for block in blocks:\n        text = '\\n'.join(r['text'] for r in block)\n        \n        if is_likely_code(text):\n            language = detect_code_language(text)\n            code_blocks.append({\n                'content': text,\n                'language': language,\n                'bbox': merge_bboxes([r['bbox'] for r in block]),\n                'confidence': sum(r['confidence'] for r in block) / len(block)\n            })\n    \n    return code_blocks\n```\n\n### IDE Detection (Bonus)\n```python\nIDE_SIGNATURES = {\n    'vscode': ['Explorer', 'TERMINAL', 'PROBLEMS', 'OUTPUT'],\n    'intellij': ['Project', 'Run', 'Debug', 'Terminal'],\n    'xcode': ['Navigator', 'Debug Area', 'Utilities'],\n}\n\ndef detect_ide(ocr_results: list[dict]) -\u003e str | None:\n    texts = [r['text'].lower() for r in ocr_results]\n    for ide, signatures in IDE_SIGNATURES.items():\n        if sum(1 for s in signatures if s.lower() in texts) \u003e= 2:\n            return ide\n    return None\n```","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:19:06.257854-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:57:08.051746-06:00","closed_at":"2026-02-01T11:57:08.051746-06:00","close_reason":"Implemented code block detection with pattern matching, pygments language detection (with heuristic fallback), IDE detection, and region clustering. 18 tests passing.","dependencies":[{"issue_id":"claudetube-mnq","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:35.90924-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-mnq","depends_on_id":"claudetube-krh","type":"blocks","created_at":"2026-01-31T23:19:44.747187-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-ose","title":"EPIC: Phase 1 - Structural Understanding","description":"Give the agent a semantic map of the video before it asks questions. Core capabilities:\n- Playlist awareness and cross-video context\n- Cheap boundary detection (transcript-first)\n- Visual scene segmentation (fallback)\n- Transcript-scene alignment\n- Visual transcripts (dense captioning)\n- Technical content extraction (OCR + code detection)\n\nThis phase transforms claudetube from 'transcript + isolated frames' to 'structured video with semantic segments'.\n\n## Success Criteria\n- [ ] Videos are automatically segmented into semantic scenes on process\n- [ ] Each scene has: start/end times, transcript chunk, keyframe paths\n- [ ] /yt:scenes command returns structured scene list\n- [ ] Scene detection works for videos without YouTube chapters\n- [ ] Cache structure supports scene-level data (scenes/ directory)","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-31T23:17:53.04411-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T09:54:36.278176-06:00","comments":[{"id":28,"issue_id":"claudetube-ose","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nAll work in this epic MUST follow the fallback hierarchy:\n1. **CACHE** - Already processed? Return immediately.\n2. **YT-DLP** - Free metadata (chapters, subtitles) from source.\n3. **LOCAL** - Fast local processing (ffprobe, transcript analysis).\n4. **COMPUTE** - Expensive operations (Whisper, PySceneDetect) only as last resort.\n\n**Never do work that's already been done. Never use expensive methods when cheap ones suffice.**\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:11:08Z"}]}
{"id":"claudetube-pow","title":"Support local embedding models","description":"## User Story\nAs a user without API access or preferring offline mode, I want local embedding models as fallback.\n\n## Acceptance Criteria\n- [ ] Works without Voyage API key\n- [ ] Uses CLIP for image embeddings\n- [ ] Uses sentence-transformers for text\n- [ ] Quality acceptable for search (not as good as Voyage)\n- [ ] Config: CLAUDETUBE_EMBEDDING_MODEL=local\n\n## Technical Implementation\n\n### Libraries\n```bash\npip install sentence-transformers  # 5M+ downloads/month\npip install open-clip-torch        # OpenAI CLIP implementation\n```\n\n### Model Choices\n\n#### Text: sentence-transformers\n```python\nfrom sentence_transformers import SentenceTransformer\n\n# Options (trade-off: quality vs speed):\n# - 'all-MiniLM-L6-v2': 384d, fast, good quality (RECOMMENDED)\n# - 'all-mpnet-base-v2': 768d, slower, best quality\n# - 'multi-qa-MiniLM-L6-cos-v1': optimized for Q\u0026A\n\ntext_model = SentenceTransformer('all-MiniLM-L6-v2')\n\ndef embed_text_local(text: str) -\u003e np.ndarray:\n    return text_model.encode(text, normalize_embeddings=True)\n```\n\n#### Image: OpenCLIP\n```python\nimport open_clip\nimport torch\nfrom PIL import Image\n\n# Options:\n# - 'ViT-B-32': 512d, fast, good (RECOMMENDED)\n# - 'ViT-L-14': 768d, slower, better\n# - 'ViT-H-14': 1024d, slow, best\n\nmodel, _, preprocess = open_clip.create_model_and_transforms(\n    'ViT-B-32', \n    pretrained='openai'\n)\nmodel.eval()\n\ndef embed_image_local(image_path: str) -\u003e np.ndarray:\n    image = preprocess(Image.open(image_path)).unsqueeze(0)\n    with torch.no_grad():\n        embedding = model.encode_image(image)\n    return embedding.squeeze().numpy()\n```\n\n### Combined Embedding Strategy\n```python\ndef embed_scene_local(scene: dict, keyframe_paths: list[str]) -\u003e np.ndarray:\n    '''Combine text and image embeddings.'''\n    \n    # Text embedding\n    text = ' '.join([\n        scene.get('transcript_text', ''),\n        scene.get('visual', {}).get('description', ''),\n    ])\n    text_emb = embed_text_local(text)  # 384d\n    \n    # Image embeddings (average of keyframes)\n    if keyframe_paths:\n        img_embs = [embed_image_local(p) for p in keyframe_paths[:3]]\n        img_emb = np.mean(img_embs, axis=0)  # 512d\n    else:\n        img_emb = np.zeros(512)\n    \n    # Concatenate: 384 + 512 = 896 dimensions\n    combined = np.concatenate([text_emb, img_emb])\n    \n    # L2 normalize for cosine similarity\n    return combined / np.linalg.norm(combined)\n```\n\n### Quality Comparison\n| Model | Text Quality | Image Quality | Speed |\n|-------|-------------|---------------|-------|\n| Voyage multimodal-3 | Excellent | Excellent | ~100ms/scene |\n| Local (MiniLM + CLIP) | Good | Good | ~50ms/scene |\n\nLocal is ~85% as accurate as Voyage for semantic search.","status":"open","priority":3,"issue_type":"feature","created_at":"2026-01-31T23:20:12.644952-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:29:48.879703-06:00","dependencies":[{"issue_id":"claudetube-pow","depends_on_id":"claudetube-dth","type":"parent-child","created_at":"2026-01-31T23:20:24.996542-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-qgi","title":"Detect linguistic transition cues in transcript","description":"## User Story\nAs a user analyzing educational content, I want automatic detection of topic transitions from speech patterns ('next let's talk about...').\n\n## Acceptance Criteria\n- [ ] Detects explicit transitions: 'next let's', 'moving on', 'now that'\n- [ ] Detects section markers: 'step 1', 'part 2', 'in this section'\n- [ ] Detects topic shifts: 'another thing', 'the next step'\n- [ ] Returns timestamp + trigger_text + confidence (0.7)\n- [ ] Low false positive rate\n\n## Technical Implementation\n\n### Library: Just regex (stdlib)\nSpeech patterns are consistent enough that ML is overkill here.\n\n```python\nimport re\nfrom typing import NamedTuple\n\nclass Boundary(NamedTuple):\n    timestamp: float\n    type: str\n    trigger_text: str\n    confidence: float\n\nTRANSITION_PATTERNS = [\n    # Explicit transitions\n    r'\\b(next|now)\\s+(let\\'?s|we(\\'ll)?|i(\\'ll)?)\\b',\n    r'\\b(moving on|let\\'s move|let\\'s talk about)\\b',\n    r'\\bnow\\s+(that|we|i)\\b',\n    r'\\b(first|second|third|finally|lastly)\\b',\n    r'\\bso\\s+(now|let\\'s|we)\\b',\n    r'\\b(okay|alright|all right)\\s*,?\\s*(so|now|let\\'s)\\b',\n    \n    # Section markers\n    r'\\b(step\\s+\\d+|part\\s+\\d+)\\b',\n    r'\\bin\\s+this\\s+(section|part|video)\\b',\n    r'\\b(to\\s+summarize|in\\s+summary|to\\s+recap)\\b',\n    \n    # Topic shifts\n    r'\\b(another\\s+(thing|way|approach|important))\\b',\n    r'\\b(the\\s+(next|last|final)\\s+(thing|step|part))\\b',\n]\n\n# Compile once for performance\nCOMPILED_PATTERNS = [re.compile(p, re.IGNORECASE) for p in TRANSITION_PATTERNS]\n\ndef detect_linguistic_boundaries(transcript_segments: list[dict]) -\u003e list[Boundary]:\n    boundaries = []\n    for seg in transcript_segments:\n        text = seg['text']\n        for pattern in COMPILED_PATTERNS:\n            if pattern.search(text):\n                boundaries.append(Boundary(\n                    timestamp=seg['start'],\n                    type='linguistic_cue',\n                    trigger_text=text[:50],\n                    confidence=0.7\n                ))\n                break  # One match per segment\n    return boundaries\n```\n\n### Performance\n- Pre-compiled regex = fast\n- O(n * p) where n=segments, p=patterns\n- Typical 30-min video: \u003c10ms","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:18:26.635596-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:08:55.549439-06:00","closed_at":"2026-02-01T11:08:55.549439-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-qgi","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:34.935694-06:00","created_by":"danielbarrett"}],"comments":[{"id":26,"issue_id":"claudetube-qgi","author":"danielbarrett","text":"Commit: ee82b127c8e72dcfc3e15a93fe6be56cd4284e6f","created_at":"2026-02-01T17:08:37Z"},{"id":27,"issue_id":"claudetube-qgi","author":"danielbarrett","text":"## What was done\n- Created `src/claudetube/analysis/` module for boundary detection\n- Implemented `detect_linguistic_boundaries()` with 11 pre-compiled regex patterns\n- Boundary namedtuple returns: timestamp, type, trigger_text, confidence (0.7)\n- Added 39 comprehensive tests covering all pattern categories and edge cases\n- Files: src/claudetube/analysis/__init__.py, src/claudetube/analysis/linguistic.py, tests/test_linguistic.py\n\n## Left undone\n- None - all acceptance criteria met\n\n## Gotchas\n- Using word boundary \\b prevents false positives from embedded words (e.g., 'renown' won't match 'now')\n- One match per segment prevents over-detection when multiple cues appear together\n- trigger_text truncated to 50 chars to keep output manageable","created_at":"2026-02-01T17:08:49Z"},{"id":36,"issue_id":"claudetube-qgi","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nLinguistic cue detection is part of the CHEAP transcript analysis chain:\n- Input: Already-fetched transcript text (no additional I/O)\n- Processing: Regex/keyword matching for transition phrases\n- Target latency: \u003c0.5s for 30-min video\n\nThis runs BEFORE any visual analysis. Cache results in scenes.json.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:11:57Z"}]}
{"id":"claudetube-quz","title":"Create unified config loader with priority resolution","description":"## User Story\nAs a developer, I need a clean API to get the resolved cache directory considering all config sources.\n\n## Acceptance Criteria\n- [ ] Single function: get_config() -\u003e ClaudetubeConfig\n- [ ] Resolves config from all sources in priority order\n- [ ] Caches result (don't re-read files every call)\n- [ ] Provides get_cache_dir() convenience function\n- [ ] Logs which config source was used\n\n## Priority Order\n1. Environment variable (CLAUDETUBE_CACHE_DIR)\n2. Project config (.claudetube/config.yaml)\n3. User config (~/.config/claudetube/config.yaml)\n4. Default (~/.claude/video_cache)\n\n## API\n```python\nfrom claudetube.config import get_cache_dir, get_config\n\ncache = get_cache_dir()  # Returns Path\n\nconfig = get_config()\nprint(config.cache_dir)\nprint(config.source)  # 'env', 'project', 'user', 'default'\n```\n\n## Files to create/modify\n- src/claudetube/config/loader.py (new)\n- src/claudetube/config/__init__.py","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-01T10:03:53.812745-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T10:54:12.299536-06:00","closed_at":"2026-02-01T10:54:12.299536-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-quz","depends_on_id":"claudetube-dug","type":"parent-child","created_at":"2026-02-01T10:30:37.312019-06:00","created_by":"danielbarrett"}],"comments":[{"id":20,"issue_id":"claudetube-quz","author":"danielbarrett","text":"## What was done\n- Created `src/claudetube/config/loader.py` with unified config loader\n- Implemented `get_config()` returning `ClaudetubeConfig` with `cache_dir` and `source`\n- Implemented `get_cache_dir()` convenience function\n- Priority resolution: env → project → user → default\n- Results cached via `lru_cache`, `clear_config_cache()` to reset\n- Added `ConfigSource` enum (env, project, user, default)\n- Updated `config/__init__.py` to export new API\n- Files: loader.py (new), __init__.py (updated)\n\n## Left undone\n- None\n\n## Gotchas\n- PyYAML import is optional - gracefully skips YAML configs if not installed\n- Project config search walks up from cwd to find nearest .claudetube/config.yaml\n\nCommit: 5785b01","created_at":"2026-02-01T16:54:05Z"}]}
{"id":"claudetube-sa5","title":"EPIC: Phase 3 - Temporal Reasoning","description":"Understand change over time, not just static moments. Core capabilities:\n- Entity tracking (people, objects, concepts, code evolution)\n- Change detection between scenes\n- Narrative structure detection (intro, sections, conclusion)\n\nThis phase enables questions like 'How did the auth middleware evolve during this video?'\n\n## Success Criteria\n- [ ] Entities (people, objects, code) tracked across scenes with IDs\n- [ ] Change summaries generated between consecutive scenes\n- [ ] Narrative structure detected (intro, main sections, conclusion)\n- [ ] Entity timeline queryable via API\n- [ ] Code evolution trackable in programming tutorials","status":"open","priority":2,"issue_type":"epic","created_at":"2026-01-31T23:18:01.543928-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T09:54:44.287306-06:00","comments":[{"id":41,"issue_id":"claudetube-sa5","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nTemporal reasoning builds on ALREADY-CACHED scene data:\n1. **CACHE** - Entity tracking results cached? Use them.\n2. **SCENES** - Use existing scene boundaries (don't recompute).\n3. **INCREMENTAL** - Track entities across scenes using cached keyframes.\n\nDon't re-extract frames or re-analyze scenes. Build on the cheap foundation.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:12:35Z"}]}
{"id":"claudetube-t10","title":"Detect significant pauses as boundaries","description":"## User Story\nAs a user, I want significant pauses (\u003e2 seconds) detected as potential topic boundaries.\n\n## Acceptance Criteria\n- [ ] Detects gaps \u003e2 seconds between transcript segments\n- [ ] Longer pauses = higher confidence (0.5 base + 0.03/sec up to 0.8)\n- [ ] Returns timestamp + gap duration\n- [ ] Works with SRT timestamp format\n\n## Technical Implementation\n\n### Library: pysrt (for SRT parsing)\n```bash\npip install pysrt  # 500k+ downloads/month\n```\n\n```python\nimport pysrt\n\ndef detect_pause_boundaries(srt_path: str) -\u003e list[dict]:\n    subs = pysrt.open(srt_path)\n    boundaries = []\n    \n    for i in range(1, len(subs)):\n        prev_end = subs[i-1].end.ordinal / 1000  # ms to seconds\n        curr_start = subs[i].start.ordinal / 1000\n        gap = curr_start - prev_end\n        \n        if gap \u003e 2.0:\n            confidence = min(0.5 + (gap * 0.03), 0.8)\n            boundaries.append({\n                'timestamp': curr_start,\n                'type': 'pause',\n                'gap_seconds': gap,\n                'confidence': confidence\n            })\n    \n    return boundaries\n```\n\n### Alternative: Parse SRT manually\nIf avoiding dependency:\n```python\nimport re\n\nSRT_TIME_PATTERN = r'(\\d{2}):(\\d{2}):(\\d{2}),(\\d{3})'\n\ndef parse_srt_timestamp(ts: str) -\u003e float:\n    h, m, s, ms = map(int, re.match(SRT_TIME_PATTERN, ts).groups())\n    return h*3600 + m*60 + s + ms/1000\n```\n\n### Why Pauses Matter\n- Speaker naturally pauses between topics\n- Editing often adds pauses at transitions\n- Low confidence because pauses can be incidental","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:18:31.386936-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:12:53.30584-06:00","closed_at":"2026-02-01T11:12:53.30584-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-t10","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:35.060405-06:00","created_by":"danielbarrett"}],"comments":[{"id":34,"issue_id":"claudetube-t10","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nPause detection is part of the CHEAP transcript analysis chain:\n- Input: Already-fetched transcript (no additional I/O)\n- Processing: Simple gap detection in timestamps\n- Target latency: \u003c0.5s for 30-min video\n\nThis runs BEFORE any visual analysis. Cache results in scenes.json.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:11:48Z"},{"id":44,"issue_id":"claudetube-t10","author":"danielbarrett","text":"## What was done\n- Added `src/claudetube/analysis/pause.py` with SRT parsing and pause detection\n- Manual SRT timestamp parsing (regex-based, no new dependencies)\n- Detects gaps \u003e2 seconds as boundaries\n- Confidence: 0.5 base + 0.03/second, capped at 0.8\n- Reuses `Boundary` NamedTuple from linguistic module\n- 35 tests in `tests/test_pause.py`\n- Files: `src/claudetube/analysis/pause.py`, `src/claudetube/analysis/__init__.py`, `tests/test_pause.py`\n\n## Left undone\n- None\n\n## Gotchas\n- Python's banker's rounding: 0.575 rounds to 0.57, not 0.58\n- SRT uses comma for milliseconds (00:00:01,000) not period\n\nCommit: 1edfda833f7db6c3568b70d2218c6443593a0643","created_at":"2026-02-01T17:12:45Z"}]}
{"id":"claudetube-u1l","title":"Support user-level ~/.config/claudetube/config.yaml","description":"## User Story\nAs a user, I want to set a global default cache directory in my home config.\n\n## Acceptance Criteria\n- [ ] Check ~/.config/claudetube/config.yaml\n- [ ] Support same config format as project config\n- [ ] Lower priority than project config and env var\n- [ ] Create example config on first run (optional)\n\n## Config Location\n- Linux/macOS: ~/.config/claudetube/config.yaml\n- Windows: %APPDATA%/claudetube/config.yaml\n\n## Files to modify\n- src/claudetube/config/defaults.py (or new config/loader.py)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-02-01T10:03:49.813934-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:45:37.586149-06:00","closed_at":"2026-02-01T11:45:37.586149-06:00","close_reason":"Implemented Windows (%APPDATA%) and XDG_CONFIG_HOME support. Added 7 tests covering Unix, Windows, and priority ordering.","dependencies":[{"issue_id":"claudetube-u1l","depends_on_id":"claudetube-quz","type":"blocks","created_at":"2026-02-01T10:29:43.656577-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-u1l","depends_on_id":"claudetube-dug","type":"parent-child","created_at":"2026-02-01T10:30:37.183861-06:00","created_by":"danielbarrett"}]}
{"id":"claudetube-u4h","title":"EPIC: Local File Support","description":"Enable claudetube to process local video files (not just URLs). Local files should be moved/symlinked to ~/.claude/video_cache and processed with the same pipeline: metadata extraction via ffprobe, transcription via whisper, frame extraction via ffmpeg. This unlocks offline workflows and processing of screen recordings, downloaded videos, etc.\n\n## Success Criteria\n- [ ] process_video() accepts local file paths (not just URLs)\n- [ ] Local files get deterministic video_id based on path hash\n- [ ] Metadata extracted via ffprobe (duration, resolution, codec)\n- [ ] Transcription works via whisper (no yt-dlp subtitles)\n- [ ] Frame extraction works via ffmpeg\n- [ ] MCP tool available for local file processing\n- [ ] Same cache structure as URL-sourced videos","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-31T23:16:00.738575-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T09:54:40.527051-06:00","comments":[{"id":29,"issue_id":"claudetube-u4h","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nLocal file processing MUST follow the fallback hierarchy:\n1. **CACHE** - Already processed? Return immediately.\n2. **EMBEDDED** - Check for embedded subtitles in container (ffprobe).\n3. **SIDECAR** - Check for .srt/.vtt files alongside video.\n4. **WHISPER** - Run transcription only as last resort.\n\nFor metadata: ffprobe is cheap, use it. For frames: cache aggressively.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:11:19Z"}]}
{"id":"claudetube-uh7","title":"Add /yt:find command","description":"## User Story\nAs a Claude Code user, I want a /yt:find command to quickly locate specific moments in videos.\n\n## Acceptance Criteria\n- [ ] MCP tool: find_moments_tool(video_id, query, top_k)\n- [ ] Returns formatted results for Claude\n- [ ] Includes timestamps for easy jumping\n- [ ] Works with processed videos\n\n## Technical Implementation\n\n### MCP Tool\n```python\n@mcp.tool()\ndef find_moments_tool(\n    video_id: str,\n    query: str,\n    top_k: int = 5\n) -\u003e str:\n    '''Find moments in a video matching a natural language query.\n    \n    Example: find_moments_tool('abc123', 'when they fix the auth bug')\n    \n    Returns ranked list of relevant moments with timestamps.\n    '''\n    \n    try:\n        moments = find_moments(video_id, query, top_k)\n    except ValueError as e:\n        return json.dumps({'error': str(e)})\n    \n    # Format for Claude\n    output = {\n        'video_id': video_id,\n        'query': query,\n        'results': moments,\n        'formatted': format_moments_for_claude(moments)\n    }\n    \n    return json.dumps(output, indent=2)\n\ndef format_moments_for_claude(moments: list[dict]) -\u003e str:\n    '''Format moments as readable text.'''\n    lines = ['Found {} relevant moments:\\n'.format(len(moments))]\n    \n    for m in moments:\n        lines.append(\n            f\"{m['rank']}. [{m['timestamp_str']}-{format_timestamp(m['end'])}] \"\n            f\"(relevance: {m['relevance']:.0%})\\n\"\n            f\"   {m['preview']}\\n\"\n        )\n    \n    return '\\n'.join(lines)\n```\n\n### Skill Command\n```\n/yt:find \u003cvideo_id\u003e \u003cquery\u003e\n```\n\n### Example Output\n```\nFound 3 relevant moments:\n\n1. [4:32-5:15] (relevance: 92%)\n   \"...so the issue was we weren't validating the token expiry...\"\n\n2. [12:08-12:45] (relevance: 85%)\n   \"...and that's how we patched the auth middleware...\"\n\n3. [8:22-8:50] (relevance: 78%)\n   \"...let me show you the failing test for authentication...\"\n```\n\n### Integration with Frame Extraction\nAfter finding moments, Claude can use get_frames_at() to examine:\n```python\n# Find moment\nmoments = find_moments('abc123', 'the code with the bug')\n\n# Examine visually\nframes = get_frames_at('abc123', moments[0]['start'], duration=10)\n```","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:20:08.410887-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T12:28:50.540698-06:00","closed_at":"2026-02-01T12:28:50.540698-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-uh7","depends_on_id":"claudetube-dth","type":"parent-child","created_at":"2026-01-31T23:20:24.87294-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-uh7","depends_on_id":"claudetube-0oq","type":"blocks","created_at":"2026-01-31T23:20:25.432926-06:00","created_by":"danielbarrett"}],"comments":[{"id":43,"issue_id":"claudetube-uh7","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\n/yt:find should feel instant for common cases:\n1. **CACHE** - Same query asked before? Return cached result.\n2. **CHAPTERS** - If query matches chapter title, return that timestamp.\n3. **TRANSCRIPT** - Text search in transcript (~100ms).\n4. **EMBEDDINGS** - Semantic search only when text fails.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:12:44Z"},{"id":67,"issue_id":"claudetube-uh7","author":"danielbarrett","text":"Commit: 035c568d043abf8431e9e984c3d5655eb2ad6425","created_at":"2026-02-01T18:28:33Z"},{"id":68,"issue_id":"claudetube-uh7","author":"danielbarrett","text":"## What was done\n- Added find_moments_tool MCP tool that wraps the existing search infrastructure\n- Added _format_moments_for_claude helper that formats search results as readable text\n- Added /yt:find skill command (commands/yt/find.md) for quick moment search\n- Added comprehensive tests for the new MCP tool\n\n## Files modified\n- src/claudetube/mcp_server.py - MCP tool and formatter\n- tests/test_mcp_server.py - New tests\n- commands/yt/find.md - Skill command\n\n## Left undone\n- None\n\n## Gotchas\n- The search infrastructure already existed in analysis/search.py - this task just exposed it via MCP\n- Format follows existing pattern: JSON with both raw results and formatted text","created_at":"2026-02-01T18:28:43Z"}]}
{"id":"claudetube-uzo","title":"Build vector index with ChromaDB","description":"## User Story\nAs a user searching videos, I need embeddings stored in a fast searchable index.\n\n## Acceptance Criteria\n- [ ] Stores scene embeddings in vector database\n- [ ] Supports semantic similarity search\n- [ ] Includes metadata: timestamps, transcript preview\n- [ ] Persistent storage in cache directory\n- [ ] Sub-second query time\n\n## Technical Implementation\n\n### Library: ChromaDB (Recommended)\n```bash\npip install chromadb  # 10M+ downloads/month, embedded vector DB\n```\n\n```python\nimport chromadb\nfrom chromadb.config import Settings\n\ndef build_scene_index(video_id: str, scenes: list, embeddings: list, cache_dir: Path):\n    '''Create searchable index of video scenes.'''\n    \n    # Persistent storage in video cache\n    db_path = cache_dir / 'embeddings' / 'chroma'\n    client = chromadb.PersistentClient(\n        path=str(db_path),\n        settings=Settings(anonymized_telemetry=False)\n    )\n    \n    # Create or get collection\n    collection = client.get_or_create_collection(\n        name='scenes',\n        metadata={'video_id': video_id}\n    )\n    \n    # Add scenes\n    collection.add(\n        ids=[f'scene_{s[\"segment_id\"]}' for s in scenes],\n        embeddings=embeddings,\n        metadatas=[{\n            'start': s['start'],\n            'end': s['end'],\n            'transcript': s.get('transcript_text', '')[:500],\n            'visual': s.get('visual', {}).get('description', '')[:500],\n        } for s in scenes],\n        documents=[s.get('transcript_text', '') for s in scenes]\n    )\n    \n    return collection\n\ndef load_scene_index(cache_dir: Path):\n    '''Load existing index.'''\n    db_path = cache_dir / 'embeddings' / 'chroma'\n    if not db_path.exists():\n        return None\n    \n    client = chromadb.PersistentClient(path=str(db_path))\n    return client.get_collection('scenes')\n```\n\n### Alternative: FAISS (Faster, but less features)\n```bash\npip install faiss-cpu  # Facebook's vector search\n```\n\n```python\nimport faiss\n\ndef build_faiss_index(embeddings: np.ndarray) -\u003e faiss.IndexFlatIP:\n    '''Build FAISS index for inner product (cosine) search.'''\n    # Normalize for cosine similarity\n    faiss.normalize_L2(embeddings)\n    \n    index = faiss.IndexFlatIP(embeddings.shape[1])\n    index.add(embeddings)\n    return index\n```\n\n### Recommendation: ChromaDB\n- Easier API (includes metadata, documents)\n- Persistent by default\n- Good enough performance for single-video search\n- FAISS only if searching across many videos\n\n### Query Function\n```python\ndef search_scenes(collection, query_embedding: np.ndarray, top_k: int = 5):\n    results = collection.query(\n        query_embeddings=[query_embedding.tolist()],\n        n_results=top_k,\n        include=['metadatas', 'documents', 'distances']\n    )\n    return results\n```","status":"closed","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:20:00.043153-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T12:16:16.374262-06:00","closed_at":"2026-02-01T12:16:16.374262-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-uzo","depends_on_id":"claudetube-dth","type":"parent-child","created_at":"2026-01-31T23:20:24.647246-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-uzo","depends_on_id":"claudetube-kvk","type":"blocks","created_at":"2026-01-31T23:20:25.21603-06:00","created_by":"danielbarrett"}],"comments":[{"id":63,"issue_id":"claudetube-uzo","author":"danielbarrett","text":"Commit: 1ee1bc1261815468e3ac5b752947f954737d89fd","created_at":"2026-02-01T18:15:59Z"},{"id":64,"issue_id":"claudetube-uzo","author":"danielbarrett","text":"## What was done\n- Added chromadb\u003e=0.4.0 as optional dependency in [search] extras\n- Created src/claudetube/analysis/vector_index.py with:\n  - build_scene_index(): Stores scene embeddings in ChromaDB\n  - load_scene_index(): Loads existing index\n  - search_scenes(): Semantic similarity search by embedding\n  - search_scenes_by_text(): Text query → embedding → search\n  - has_vector_index(), delete_scene_index(), get_index_stats()\n  - SearchResult dataclass with scene metadata\n- Exported all functions from analysis/__init__.py\n- Created tests/test_vector_index.py with 18 tests\n\n## Left undone\n- None (all acceptance criteria met)\n\n## Gotchas\n- ChromaDB tests skip gracefully when package not installed\n- Uses contextlib.suppress for cleaner exception handling\n- voyage-3 model for text queries, voyage-multimodal-3 for scenes","created_at":"2026-02-01T18:16:10Z"}]}
{"id":"claudetube-vs1","title":"Smart segmentation strategy","description":"## User Story\nAs a developer, I need a smart entry point that tries cheap methods first and only falls back to visual detection when necessary.\n\n## Acceptance Criteria\n- [ ] Tries cheap methods first (always)\n- [ ] Evaluates boundary coverage\n- [ ] Falls back to visual only when needed\n- [ ] Skips visual if good chapters exist (\u003e5 chapters)\n- [ ] Converts boundaries to segments with start/end times\n- [ ] Saves to scenes/scenes.json\n\n## Technical Implementation\n\n### Strategy Pattern\n```python\nfrom dataclasses import dataclass\nfrom pathlib import Path\nimport json\n\n@dataclass\nclass SegmentationResult:\n    segments: list\n    method: str  # 'transcript' or 'visual'\n    boundary_count: int\n    avg_segment_duration: float\n\ndef segment_video_smart(\n    video_id: str,\n    video_path: str,\n    transcript_segments: list,\n    video_info: dict,\n    output_dir: Path\n) -\u003e SegmentationResult:\n    duration = video_info.get('duration', 0)\n    \n    # Step 1: Always try cheap methods first\n    cheap = detect_boundaries_cheap(video_info, transcript_segments, \n                                     output_dir / 'audio.srt')\n    \n    # Step 2: Evaluate coverage\n    has_chapters = any(b.type == 'chapter' for b in cheap)\n    avg_segment = duration / (len(cheap) + 1) if cheap else duration\n    \n    # Step 3: Decide if visual needed\n    need_visual = should_use_visual_detection(cheap, duration, bool(transcript_segments))\n    \n    # Step 4: Skip visual if good chapters exist\n    if has_chapters and len(cheap) \u003e= 5:\n        need_visual = False\n    \n    # Step 5: Run visual if needed, merge\n    if need_visual:\n        visual = detect_visual_boundaries(video_path)\n        all_boundaries = merge_boundary_sources(cheap, visual)\n        method = 'visual'\n    else:\n        all_boundaries = cheap\n        method = 'transcript'\n    \n    # Step 6: Convert to segments\n    segments = boundaries_to_segments(all_boundaries, duration)\n    \n    # Step 7: Save to cache\n    scenes_dir = output_dir / 'scenes'\n    scenes_dir.mkdir(exist_ok=True)\n    \n    result = {\n        'segments': [asdict(s) for s in segments],\n        'method': method,\n        'boundary_count': len(all_boundaries)\n    }\n    (scenes_dir / 'scenes.json').write_text(json.dumps(result, indent=2))\n    \n    return SegmentationResult(\n        segments=segments,\n        method=method,\n        boundary_count=len(all_boundaries),\n        avg_segment_duration=duration / len(segments)\n    )\n```\n\n### Boundary to Segment Conversion\n```python\n@dataclass\nclass Segment:\n    segment_id: int\n    start: float\n    end: float\n    boundary_info: dict | None\n\ndef boundaries_to_segments(boundaries: list, duration: float) -\u003e list[Segment]:\n    if not boundaries:\n        return [Segment(0, 0, duration, None)]\n    \n    sorted_b = sorted(boundaries, key=lambda x: x.timestamp)\n    segments = []\n    \n    # First segment: 0 to first boundary\n    segments.append(Segment(0, 0, sorted_b[0].timestamp, None))\n    \n    # Middle segments\n    for i, b in enumerate(sorted_b[:-1]):\n        segments.append(Segment(i+1, b.timestamp, sorted_b[i+1].timestamp, asdict(b)))\n    \n    # Last segment: last boundary to end\n    segments.append(Segment(len(sorted_b), sorted_b[-1].timestamp, duration, asdict(sorted_b[-1])))\n    \n    return segments\n```","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-31T23:18:48.973507-06:00","created_by":"danielbarrett","updated_at":"2026-02-01T11:37:20.607769-06:00","closed_at":"2026-02-01T11:37:20.607769-06:00","close_reason":"Done","dependencies":[{"issue_id":"claudetube-vs1","depends_on_id":"claudetube-ose","type":"parent-child","created_at":"2026-01-31T23:19:35.491909-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-vs1","depends_on_id":"claudetube-3dt","type":"blocks","created_at":"2026-01-31T23:19:44.170962-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-vs1","depends_on_id":"claudetube-9f9","type":"blocks","created_at":"2026-01-31T23:19:44.280917-06:00","created_by":"danielbarrett"}],"comments":[{"id":31,"issue_id":"claudetube-vs1","author":"danielbarrett","text":"## Architecture Principle: Cheap First, Expensive Last\n\nTHIS IS THE CORE IMPLEMENTATION of the fallback hierarchy:\n1. **CACHE** - scenes/scenes.json exists? Return it.\n2. **CHAPTERS** - YouTube chapters from yt-dlp metadata (FREE, instant).\n3. **DESCRIPTION** - Parse timestamps from video description (FREE, instant).\n4. **TRANSCRIPT** - Analyze pauses, vocabulary shifts (~1-2s).\n5. **VISUAL** - PySceneDetect ONLY if coverage is poor (\u003c5 boundaries for 30min video).\n\nTarget: \u003c2s for 30-min video when chapters exist. Skip visual detection entirely if good chapters found.\n\nSee: documentation/architecture/principles.md","created_at":"2026-02-01T17:11:27Z"},{"id":50,"issue_id":"claudetube-vs1","author":"danielbarrett","text":"## What was done\n- Implemented `segment_video_smart()` function that orchestrates the cheap-first segmentation strategy\n- Implemented `boundaries_to_segments()` to convert boundaries to segments with start/end times\n- Added cache check: returns cached scenes/scenes.json if exists\n- Evaluates boundary coverage and skips visual detection if \u003e=5 chapters found\n- Saves results to scenes/scenes.json\n\nFiles:\n- src/claudetube/operations/segmentation.py (new - 210 lines)\n- src/claudetube/operations/__init__.py (exports added)\n- tests/test_segmentation.py (17 tests, all passing)\n\n## Left undone\n- None\n\n## Gotchas\n- Title extraction from chapter trigger_text requires parsing \"[source] Title\" format\n- Boundary at video start (\u003c0.5s) doesn't create an empty first segment\n- Default duration of 3600s used when metadata unavailable","created_at":"2026-02-01T17:37:12Z"}]}
{"id":"claudetube-zs7","title":"Detect changes between consecutive scenes","description":"## User Story\nAs a user analyzing video flow, I want to know what changed between consecutive scenes.\n\n## Acceptance Criteria\n- [ ] Detects visual changes: objects added/removed\n- [ ] Detects topic shift via embedding similarity\n- [ ] Detects content type changes (code → slides)\n- [ ] Stores in structure/changes.json\n\n## Technical Implementation\n\n### Simple Set-Based Change Detection\n```python\nfrom dataclasses import dataclass\nimport numpy as np\n\n@dataclass\nclass SceneChanges:\n    scene_a_id: int\n    scene_b_id: int\n    visual_changes: dict\n    topic_shift_score: float\n    content_type_change: bool\n\ndef detect_changes(scene_a: dict, scene_b: dict) -\u003e SceneChanges:\n    '''Identify what changed between consecutive scenes.'''\n    \n    # Visual element changes\n    elements_a = set(scene_a.get('visual', {}).get('objects', []))\n    elements_b = set(scene_b.get('visual', {}).get('objects', []))\n    \n    visual_changes = {\n        'added': list(elements_b - elements_a),\n        'removed': list(elements_a - elements_b),\n        'persistent': list(elements_a \u0026 elements_b)\n    }\n    \n    # Content type change\n    type_a = scene_a.get('technical', {}).get('content_type', 'unknown')\n    type_b = scene_b.get('technical', {}).get('content_type', 'unknown')\n    content_type_change = type_a != type_b\n    \n    # Topic shift via embedding similarity\n    emb_a = np.array(scene_a.get('embedding', []))\n    emb_b = np.array(scene_b.get('embedding', []))\n    \n    if emb_a.size and emb_b.size:\n        topic_shift = 1 - cosine_similarity(emb_a, emb_b)\n    else:\n        topic_shift = 0.0\n    \n    return SceneChanges(\n        scene_a_id=scene_a['segment_id'],\n        scene_b_id=scene_b['segment_id'],\n        visual_changes=visual_changes,\n        topic_shift_score=topic_shift,\n        content_type_change=content_type_change\n    )\n\ndef cosine_similarity(a: np.ndarray, b: np.ndarray) -\u003e float:\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n```\n\n### Build Changes for Entire Video\n```python\ndef build_change_timeline(scenes: list[dict]) -\u003e list[SceneChanges]:\n    '''Detect changes between all consecutive scenes.'''\n    \n    changes = []\n    for i in range(1, len(scenes)):\n        change = detect_changes(scenes[i-1], scenes[i])\n        changes.append(change)\n    \n    return changes\n```\n\n### Output Format\n```json\n{\n  \"changes\": [\n    {\n      \"scene_a_id\": 0,\n      \"scene_b_id\": 1,\n      \"visual_changes\": {\n        \"added\": [\"code_editor\"],\n        \"removed\": [\"title_slide\"],\n        \"persistent\": [\"presenter\"]\n      },\n      \"topic_shift_score\": 0.35,\n      \"content_type_change\": true\n    }\n  ],\n  \"summary\": {\n    \"major_transitions\": [1, 5, 12],\n    \"avg_topic_shift\": 0.22\n  }\n}\n```","status":"open","priority":2,"issue_type":"feature","created_at":"2026-01-31T23:20:44.023236-06:00","created_by":"danielbarrett","updated_at":"2026-01-31T23:30:36.30484-06:00","dependencies":[{"issue_id":"claudetube-zs7","depends_on_id":"claudetube-sa5","type":"parent-child","created_at":"2026-01-31T23:21:04.596309-06:00","created_by":"danielbarrett"},{"issue_id":"claudetube-zs7","depends_on_id":"claudetube-kvk","type":"blocks","created_at":"2026-01-31T23:21:05.197076-06:00","created_by":"danielbarrett"}]}
